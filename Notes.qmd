---
title: "ML_Notes"
number-sections: true
format: 
  html:
    css: full-width.css
execute: 
  echo: true
  eval: true
  warning: false
editor: visual
toc: true
---

# Lesson 1: Overview of SL & ML

```{r, echo = FALSE}
library(knitr)

# Create a data frame
data <- data.frame(
  Areas = c("Scope", "Focus", "Data", "Approach","Concern", "Application"),
  Statistical_Learning = c("Subfield of Statistics", 
                           "Models building and their interpretability", "Use survey methods / experimental study to collect random data with a particular purpose or objective - understand ideas behind various techniques and accurately assess performance of each technique",
                           "Models with predefined assumptions and all data. Interpretable but holds limitations in capturing complex patterns",
                           "Parameter estimation and hypothesis testing at a certain error rate",
                           "Econometrics, biostatistics, finance, etc"),
  Machine_Learning = c("Subfield of AI", 
                       "Prediction accuracy", 
                       "Collect large or big data set in a routine way - Focuses on large scale applications",
                       "Use training data to build the model with no assumptions and use a test set to evaluation model",
                       "Bias-variance trade-off and prediction errors",
                       "Natural language proccessing, face recognition, traffic prediction, where predictive accuracy and pattern recognition are paramount")
)

# Create a table
kable(data, caption = "Statistical Learning VS Machine Learning")
```

```{r, echo = FALSE}
# Create a data frame
data <- data.frame(
  Areas = c("Focus", "Variables", "Purpose", "Concern"),
  Supervised_Learning = c("Focus on outcome measurement, Y (dependent variable)", 
                           "Use p predictor measurements (independent variables)", 
                          "For regression / classification problems",
                          "-")
                          ,
  Unsupervised_Learning = c("No outcome measurement, Y, just a set of predictors", 
                            "Find features of X that behave similarly or find linear combinations of X with the most variation", 
                            "Use unsupervised learning as first step of supervised learning",
                            "Difficult to evaluate the performance of approaches because of no outcome measurement for comparisons"
                       )
)

# Create a table
kable(data, caption = "Supervised learning VS Unsupervised learning")
```

## Some Basic Terminologies

-   Reducible Error $$
    f(X) - \hat{f}(X)
    $$ where $f(X)$ is the true value while $\hat{f}(x)$ is the predicted value

    -   We seek to find the most appropriate statistical learning technique that minimises the reducible error

-   Irreducible Error $$
    \epsilon = Y - f(X)
    $$

    -   $\epsilon$ cannot be predicted using $X$

-   Total Errors The average of the squared difference between the predicted $Y$ and actual value $Y$ is $$
    E(Y-\hat{Y})^2 = E[f(X) +\epsilon -\hat{f}(X)]^2 + Var(\epsilon)
    $$

$$
E(Y-\hat{Y})^2 = [f(X)-\hat{f}(X)]^2 + Var(\epsilon)
$$

-   Estimation of $f$

    -   Parametric
        -   Assumes a function form/shape of $f$
    -   Non-parametric
        -   Does not assume the shape of $f$

-   Training & Testing Data

    Objectives:

    -   Accurately predict unseen test cases
    -   Understand which independent variables affect the dependent variable, and how
    -   Assess quality of predictions or/and inferences

-   Prediction Accuracy VS Interpretability

    -   More restrictive (less flexibility) means more interpretable in inference objective
        -   Less flexibility, it is likely bias is higher. Vice Versa
        -   Less flexibility, it is likely variance is smaller. Vice Versa
    -   For prediction objective, less restrictive does not always yield the best model.

![](images/Lesson1_img1.png)

-   

    ```         
    Bias Variance Trade Off
    ```

    $$
    E(y_0-\hat{f}(x_0))^2 = Var(\hat{f}(x_0))+[Bias(\hat{f}(x_0))]^2+Var(\epsilon)
    $$

    -   Bias: Error that is introduced by approximating a real-life situation with a much simpler model $\hat{f}$
    -   Variance: Amount by which $\hat{f}$ would change if we estimated it again using a different training data set
    -   Relationship with flexibility: As flexibility increases, its variance increases while its bias decreases. ![](images/Lesson1_img2.png)

-   Validation

    -   Model is fitted on a training data set and tested with a validation set.
    -   The validation test set provides a validation-set error which provides an estimate of the test error

-   K-Fold Cross Validation

    -   Widely used approach for estimating test error
    -   Randomly divide the data into $K$ equal-sized parts
    -   Results from each K parts are combined at the end

-   Validation and Cross-Validation

    -   Use cross-validation to identify the parameter values for a given approach
    -   Use validation to choose best approach among all the considered approaches

As the foundation of this course is from Statistical learning with R, refer to [Statistical Learning Notes](file:///Users/junhaoteo/Documents/SMU_Modules/Y2S2/Statistical%20Learning%20with%20R/DSA211_notes.html) where the various codes can be obtained.

## Some basic codes:

-   Multiple regression model + Quadratic variables

```{r}
# Carseats example in textbook 
library(ISLR)     
attach(Carseats)
names(Carseats)
summary(Carseats)

#multiple regression model including all independent variables
lm.carseat1=lm(Sales~., data=Carseats)
summary(lm.carseat1)

# multiple regression model with significant factors
lm.carseat2=lm(Sales~CompPrice+Income+Advertising+Price+ShelveLoc+Age, 
               data=Carseats)
summary(lm.carseat2)

confint(lm.carseat2, level=0.95)
predict(lm.carseat2, 
        data.frame(CompPrice=116, Income=80, Advertising=7,
                                Price=100, ShelveLoc="Good", Age=56), 
        interval="confidence", 
        level=.95)
predict(lm.carseat2, 
        data.frame(CompPrice=116, Income=80, Advertising=7,
                                Price=100, ShelveLoc="Good", Age=56), 
        interval="prediction", 
        level=.95)

# quadratic relationship
lm.carseat3=lm(Sales~CompPrice+Income+Advertising+
                 Price+ShelveLoc+Age+I(Price^2), data=Carseats)
summary(lm.carseat3)
```

-   Code for train-test split for multiple regression

```{r}
library(ISLR)
attach(Auto)

set.seed(3344)
Auto1 <- Auto[,1:8]
train <- sample(1:nrow(Auto1), 200) #sample 200 out of the whole sample size
test <- -train
auto.train <- Auto1[train,]
auto.test <- Auto1[test,]

# multiple regression model
lm.fit <- lm(mpg~., data=auto.train)
lm.pred <- predict(lm.fit, auto.test)
mse.mrm <- mean((auto.test$mpg-lm.pred)^2)
mse.mrm
```

-   Lasso & Ridge Regularization

```{r}
#Lasso 
library(glmnet)
train.x <- model.matrix(mpg~., data=auto.train)[,-1]      # Remove the intercept
train.y <-auto.train$mpg
test.x <- model.matrix(mpg~., data=auto.test)[,-1]        # Remove the intercept
test.y <- auto.test$mpg

lasso.mod <- glmnet(train.x, train.y, alpha=1)
cv.out <- cv.glmnet(train.x, train.y, alpha=1,
                    nfolds = 10)            # Default 10 Fold, change this for other fold value
lambda.lasso <- cv.out$lambda.min
lambda.lasso
lasso.pred <- predict(lasso.mod, newx=test.x, s=lambda.lasso)
mse.lasso <- mean((test.y-lasso.pred)^2)
mse.lasso

#Ridge regression
ridge.mod <- glmnet(train.x, train.y, alpha=0)
cv.out <- cv.glmnet(train.x, train.y, alpha=0)
lambda.rr <- cv.out$lambda.min
lambda.rr
ridge.pred <- predict(ridge.mod, newx=test.x, s=lambda.rr)
mse.rr <- mean((test.y-ridge.pred)^2)
mse.rr
```

After comparing all models above using the Auto Dataset, we have that Lasso is the best model.

Rebuild the model using all of the data:

```{r}
x <- model.matrix(mpg~., data=Auto1)[,-1]
y <- Auto1$mpg
out.lasso <- glmnet(x,y, alpha=1)
lasso.m2 <- predict(out.lasso, type="coefficients", s=lambda.lasso)[1:8,]
lasso.m2[lasso.m2!=0]
```

## Classification Setting

-   Testing Error for Classification

-   Confusion Matrix

![](images/Lesson1_img3.png)

# Lesson 2: Unsupervised Learning I

## Principal Components Analysis (PCA)

-   What is it?

    -   A dimensionality reduction method by reducing the dimensionality of large data sets
    -   Finds a sequence of linear combinations of the variables that have **maximal variance**, and **mutually uncorrelated**

$$
X_1, X_2, â€¦, X_{1000} \\
\text{The above is converted through PCA to output: } \\
Var(Y_1+Y_2) = Var(Y_1)+Var(Y_2)+2Cov(Y_1, Y_2)
$$

```         
-   Reducing dimension comes at the expense of accuracy, but essentially to trade little accuracy for simplicity
    -   Low dimension data sets are easier to explore and visualize and makes analysing data points easier and faster for machine learning
```

-   Applications of PCA

Use the principal components:

```         
- In understanding and visualising the data
- As a tool for data imputation, for filling in missing values (No time to cover)
- As predictors in a regression model, instead of original larger set of independent variables
```

-   Mathematical Representation of PCA

Each observation (row) is multiplied to the loading vector to obtain the scores, $Z$.

Loading vector $$
\phi_i = (\phi_{11}, \phi_{21},... \phi_{p1})^T
$$

Under PCA, the objective is to minimise the perpendicular distance from the observation to the line.
As such the distance of the line is maximised.

Goal: $$
\max{\frac{\sum^{n}_{i=1} Z_i^2} {n}}
$$ (To input graphs from slides)

### Second Prinipal Component

Steps are similar to the First Principal Component, however, it has to be uncorrelated with $Z_1$.

### Singular Value Decomposition

(TO ADD IN THEORY)

#### Simple Numerical Example of Singular Value Decomposition - Manual calculation

```{r}
data <- c(3, 5, 5, 9, 1, 9, 3, 7, 4, 2, 5, 9, 1, 6, 8, 3)
A <- matrix(data, nrow = 4)
A

sA <- svd(A) # Do SVD
names(sA)

d <- sA$d
U <- sA$u
V <- sA$v
D <- diag(d)
```

```{r}
round(U, 2)
round(t(V), 2)
round(D, 2)
round(t(V) %*% V, 2) # Matrix multiplication - Becomes identity matrix
round(t(U) %*% U, 2) # Matrix multiplication - Becomes identity matrix
round(U %*% D %*% t(V), 2)
```

Using the example above, we have that:

![](images/Lesson2_img1.png)

```{r}
u1 <- U[,1]
u1
v1 <- V[,1]
v1
d1 <- d[1]
d1
A1 <- u1 %*% t(v1)*d1
round(A1,2)
```

We see that A1 is still quite far from the original A matrix.
By adding another principal component we have:

![](images/Lesson2_img2.png)

```{r}
u2 <- U[,2]
u2
v2 <- V[,2]
v2
d2 <- d[2]
d2
A2 <- u2 %*% t(v2)*d2
round(A2,2)
A1A2 <- A1+A2
round(A1A2,2)
```

From the above, we see that with PC1 and PC2, it gets closer to the original A matrix.

Repeating the steps one more time.

```{r}
u3 <- U[,3]
u3
v3 <- V[,3]
v3
d3 <- d[3]
d3
A3 <- u3 %*% t(v3)*d3
round(A3,2)
A1A2A3 <- A1+A2+A3
round(A1A2A3, 2)
```

## Proportion of Variance Explained (PVE)

-   Determines the number of principal components

    -   PVEs should sum to one when maximum number of principal component is used

### Coding

Using another numerical example:

```{r}
data <- c(12,13,15, 17, 21, 24, 26, 34, 28, 29, 43, 59, 20, 35, 30, 48, 32, 56)
M <- matrix(data, nrow=6)  # data matrix with dimension 6x3
M

# Manual calculation
X <- scale(M)  # scale the matrix M, to ensure data has a mean of 0
               # The standard deviation is also adjusted to 1
round(X, 4)
sX <- svd(X) # do SVD
names(sX)
d <- sX$d
U <- sX$u
V <- sX$v
D <- diag(d)
round(U, 4)
round(V, 4)
round(D, 4)
round(t(V) %*% V, 4)
round(t(U) %*% U, 4)
round(U %*% D %*% t(V), 4)
pcs <- U %*% D
pcs
pcss <- X %*% V
pcss
```

```{r}
# Using pcob
pcob <- prcomp(M, scale=TRUE)  # use R function prcomp, with scaling
                               # Use scaling when the features are very different.
names(pcob)
summary(pcob)
pcob$sdev      # standard deviation from PCs
pcob$rotation  # same as matrix V
pcob$center    # means for scaling
pcob$scale     # sd for scaling
pcob$x        # XV or UD
totalsum <- sum(pcss^2)  # total sum
pc1sum <- sum(pcss[,1]^2) # sum from PC1
pc2sum <- sum(pcss[,2]^2) # sum from PC2
pc3sum <- sum(pcss[,3]^2) # sum from PC3
sd <- sqrt(c(pc1sum, pc2sum, pc3sum)/(6-1)) # n-1=6-1=5 
sd
totalsum
pc1sum
pc2sum
pc3sum
propvar <- c(pc1sum, pc2sum, pc3sum)/totalsum
round(propvar, 4)
par(mfrow=c(1,2))
biplot(pcob, scale=0)
plot(pcob$x[,1], pcob$x[,2], xlab="PC1", ylab="PC2", ylim=c(-2, 2),
     main="Plot PC1 vs. PC2")
```

Using the US Arrest Dataset

```{r}
# Load the USArrests data and set row names as state names
data("USArrests")
states <- row.names(USArrests)
row.names(USArrests) <- states

# Display dimensions and variable names of the dataset
# dim(USArrests)
# names(USArrests)

# Optionally, calculate means and standard deviations for each variable
# apply(USArrests, 2, mean)
# apply(USArrests, 2, sd)

# Perform PCA on the scaled data
pr.out <- prcomp(USArrests, scale = TRUE)
summary(pr.out) # 2 principal components are sufficient

# Examine the components of the PCA output
pr.out$center  # Mean of each variable
pr.out$scale   # Scaling applied to each variable
pr.out$rotation  # The rotation (loading of each variable on the principal components)
```

```{r}
# Dimensions and plot of the first two principal components
dim(pr.out$x)
biplot(pr.out, scale = 0, main = "The first two Principal Components")
```

For the above graph, the top represents PC1 score while the right scale represents PC2 score.

```{r}
# To create a mirrored version of the biplot
## The software may result in different sign for its loading vectors, the rotation converts the sign.
pr.out$rotation <- -pr.out$rotation
pr.out$x <- -pr.out$x
biplot(pr.out, scale = 0, main = "The first two Principal Components mirror image")

# Calculate and print the proportion of variance explained by each principal component
pr.var <- pr.out$sdev^2
pve <- pr.var/sum(pr.var)
round(pve, 4)

# Plotting the variance explained
par(mfrow = c(1, 2))
plot(pve, xlab = "Principal Component", ylab = "Proportion of Variance Explained", ylim = c(0, 1), type = "b")
plot(cumsum(pve), xlab = "Cumulative Principal Component", ylab = "Proportion of Variance Explained", ylim = c(0, 1), type = "b")
```

# Lesson 3: Unsupervised Learning II

## Kmeans-clustering

An approach for partitioning a dataset into $K$ distinct, non-overlapping clusters - Properties

```         
- Each observation belongs to at least one of the K clusters
- Clusters are non-overlapping: No observation belongs to more than one cluster
```

-   Within Cluster Variation $$
    \min_{C_1,...,C_K} \{\sum^K_{k=1} WCV(C_k) \} 
    $$ The formula above is to be solved such that the total within-cluster variation, summed over all $K$ clusters, is minimised.

    -   Euclidean distance calculation: $$
        WCV(C_k)= \frac{1}{|C_k|} \sum_{i,i'\epsilon C_k} \sum^p_{j=1} (x_{ij}-x_{i'j})^2
        $$ where $|C_k|$ denotes the no. of obs in the $k$th cluster.

$(x_{ij}-x_{i'j})^2$ : Represents the distance between two observation

Combining the two formulas above gives us: $$
\min_{C_1,...,C_K} \{\sum^K_{k=1} \frac{1}{|C_k|} \sum_{i,i'\epsilon C_k} \sum^p_{j=1} (x_{ij}-x_{i'j})^2 \} 
$$ Can be simplied to: $$
2\sum_{i\subset C_k}\sum^{p}_{j=1} (x_{ij}-\bar{x}_{kj})^2
$$ \$\bar{x}\_{kj} \$ : Represents the mean of the cluster The code chunk below uses the formulas above to deduce the total sum.

(Need more inputs after lesson)

```{r}
d1 <- c(12, 13, 14, 17, 20)
d2 <- c(4, 7, 10)
d3 <- c(18, 19, 22, 25, 27)

su1=0
for (i in 1: length(d1)) {
  su1 <- su1 + sum((d1[i]-d1)^2)
}
s1 <- su1/length(d1)
s1

su2=0
for (i in 1: length(d2)) {
  su2 <- su2 + sum((d2[i]-d2)^2)
}
s2 <- su2/length(d2)
s2

su3=0
for (i in 1: length(d3)) {
  su3 <- su3 + sum((d3[i]-d3)^2)
}
s3 <- su3/length(d3)
s3

tots <- s1+s2+s3
tots

sum1 <- sum((d1-mean(d1))^2)
sum2 <- sum((d2-mean(d2))^2)
sum3 <- sum((d3-mean(d3))^2)
totsum <- 2*(sum1+sum2+sum3)
c(sum1, sum2, sum3, totsum)
```

### General steps in K-clustering

1.  Randomly assign a number, from 1 to $K$, to each observation.
    These serve as initial cluster assignments.

2.  Iterate until cluster assignments stop changing:

    2.1: For each of the $K$ clusters, compute the cluster centroid.
    The $k$th cluster centroid is the vector of the $p$ feature means for the observations in the $k$th cluster.

    2.2: Assign each observation to the cluster whose centroid is closest, defined by the Euclidean distance.

Visual Representation of the steps:

![](images/Lesson3_img3.png) \#### Example Code of the steps shown above

```{r}
set.seed(2)
x <- matrix(c(12, 13, 11, 18, 16, 17, 19, 20, 21, 22), ncol=2)
# x # To view the matrix
plot(x, main="plot of X1 and X2", xlab="X1", ylab="X2", pch=20, cex=2)
k.out <- kmeans(x, centers = 2, nstart=20)  # centers : No. of clusters, nstart: No. of random sets
k.out$cluster
plot(x, col=(k.out$cluster+1), main="K-means Clustering Results with K=2",
     xlab="X1", ylab="X2", pch=20, cex=2)
k.out
```

Total variation is reduced by 81.2% by clustering them.

Below is how the variation is manually calculated.
We can see that the end result is the same as the above

```{r}
# Manual calculation of the variation:

g1 <- k.out$centers[1,]
g2 <- k.out$centers[2,]
# calculate total sum of squares
tot <- rep(NA,5)
for (i in 1:5)
  tot[i] <- (x[i,1]-mean(x[,1]))^2+(x[i,2]-mean(x[,2]))^2
sstot <- sum(tot)
sstot
# calculate the sum of squares due to group 1
totg1 <- rep(NA,2)
for (i in 4:5)
  totg1[i-3] <- (x[i,1]-g1[1])^2+(x[i,2]-g1[2])^2
ssg1 <- sum(totg1)
ssg1
# calculate the sum of squares due to group 2
totg2 <- rep(NA,3)
for (i in 1:3)
  totg2[i] <- (x[i,1]-g2[1])^2+(x[i,2]-g2[2])^2
ssg2 <- sum(totg2)
ssg2

# calculate the proportion of sum of squares explained by clusters
1-(ssg1+ssg2)/sstot
```

### Practical Application

#### Initial Visualisation of PCA

```{r}
score <- read.csv("Datasets/Score.csv", stringsAsFactors = TRUE)
summary(score)
dim(score)
attach(score)
# perform PCA
pr.out <- prcomp(score)  # we do not scale the data  
summary(pr.out)
biplot(pr.out, scale=0)
pr.out$rotation
pc1 <- pr.out$x[,1]

plot(pc1, score$Participation)
plot(pc1, score$Assignment)
plot(pc1, score$Test)
plot(pc1, score$Exam)
```

#### K-means clustering

```{r}
set.seed(123)
k.out <- kmeans(score, 4, nstart=20)  # we set 4 clusters
k.out

# Only K-means
plot(score$Exam, score$Test, col=(k.out$cluster+1), 
     main="K-means Clustering Results with K=4 based on Exam and Test scores",
     xlab="Exam", ylab="Test", pch=20, cex=2)

# With PCA
plot(pr.out$x[,1], pr.out$x[,2], col=(k.out$cluster+1), 
     main="K-means Clustering Results with K=4 based on PCA",
     xlab="PC1", ylab="PC2", pch=20, cex=2)
```

## Hierarchical clustering - (To copy over from python notes)

### Theory

-   Difference to K-Means

    -   Does not require pre-determination of clusters
    -   Results in an attractive tree-based representation
    -   Dendrogram is built starting from the leaves and combining clusters up to the trunk (bottom-up clusters)

-   Methodology (As illustrated below):

    1.  Start with each point in its own cluster
    2.  Identify the closest two clusters and merge them
    3.  Repeat Step 1 and 2
    4.  Ends when all points are in a single cluster

![](images/Lesson3_img1.png)

For the example above,

1.  We start by identifying A and C to be the closest, forming the first part of the Dendrogram
2.  Afterwards, we find the next closest, which is D and E, which forms the second part of the Dendrogram.
3.  Next, cluster AC is closest to B and therefore we merge them together to form one cluster. This is represented in the Dendrogram as well.
4.  Lastly, cluster ACB and cluster DE are merged together, forming the entire Dendrogram

### Types of linkages and its numerical example

```{r, echo = FALSE}
# Create a data frame
data <- data.frame(
  Linkage = c("Complete", "Single", "Average"),
  Description = c("Maximal inter-cluster dissimilarity. Compute all pairwise dissimilarities between the observations in cluster A and the observations in cluster B, and record the largest of these dissimilarities.",
                  "Minimal inter-cluster dissimilarity. Compute all pairwise dissimilarities between the observations. Compute all pairwise dissimilarities between the observations in cluster A and the observations in cluster B, and record the smallest of these dissimilarities.",
                  "Mean inter-cluster dissimilarity. Compute all pairwise dissimilarities between the observations in cluster A and the observations in cluster B, and record the average of these dissimilarities.")
  
)

# Create a table
kable(data, caption = "Types of Linkages")
```

Formal formulas: Complete: $$
d_{CL}(G,H) =\max_{i\space \epsilon \space G,i' \space \epsilon \space H} {d_{ii'}}
$$ Let's say:

-   Cluster 1 has 2 observations: $$
    (2, 4), (3, 5)
    $$

-   Cluster 2 has 2 observations: $$
    (4, 6), (7,6)
    $$

-   Cluster 3 has 1 observation: $$
    (5, 9)
    $$

The inter-cluster dissimilarity between Clusters 1 and 2:

-   (2, 4) and (4, 6) dissimilarity measure =$\sqrt{(2-4)^2+(4-6)^2} = \sqrt{8}$

-   (2, 4) and (7, 6) dissimilarity measure =$\sqrt{(2-7)^2+(4-6)^2 }= \sqrt{29}$

-   (3, 5) and (4, 6) dissimilarity measure =$\sqrt{(3-4)^2+(5-6)^2}=\sqrt{2}$

-   (3, 5) and (7, 6) dissimilarity measure =$\sqrt{(3-7)^2+(5-6)^2}=\sqrt{17}$

The inter-cluster dissimilarity between Clusters 2 and 3:

-   (4, 6) and (5, 9) dissimilarity measure =$\sqrt{(4-5)^2+(6-9)^2} = \sqrt{10}$

-   (7, 6) and (5, 9) dissimilarity measure =$\sqrt{(7-5)^2+(6-9)^2 }= \sqrt{13}$

The inter-cluster dissimilarity between Clusters 1 and 3:

-   (2, 4) and (5, 9) dissimilarity measure =$\sqrt{(2-5)^2+(4-9)^2} = \sqrt{34}$

-   (3, 5) and (5, 9) dissimilarity measure =$\sqrt{(3-5)^2+(5-9)^2 }= \sqrt{20}$

**Complete linkage**: Focus on farthest pair of points, producing tight, compact clusters

Therefore, inter-cluster dissimilarity between Clusters 1 and 2 = $\max(\sqrt{8},\sqrt{29},\sqrt{2},\sqrt{17})=\sqrt{29}$

**Single linkage**: Focuses on closest pair of points, resulting in elongated clusters

Therefore, inter-cluster dissimilarity between Clusters 1 and 2 = $\min(\sqrt{8},\sqrt{29},\sqrt{2},\sqrt{17})=\sqrt{2}$

**Average linkage**: Leads to balanced clustering

Therefore, inter-cluster dissimilarity between Clusters 1 and 2 = $\frac{(\sqrt{8}+\sqrt{29}+\sqrt{2}+\sqrt{17})}{4}=3.4377$

### Additional information:

-   

    ```         
    Correlation-based distance
    ```

    -   If two observations are highly correlated but they have high Euclidean distance, correlation-based distance may be prefered
        -   This measure focuses on the shapes of observation profiles rather than their magnitudes

![](images/Lesson3_img2.png)

### Example Code

```{r}
super <- read.csv("Datasets/Supermarket.csv", stringsAsFactors = TRUE)
summary(super)
# View(super)

set.seed(9876)
k.out <- kmeans(super[,-1], 3, nstart=20) # Remove the customer column
k.out
km.cluster <- k.out$cluster
```

Note that under Dendrogram, the function *cutree(tree, cluster)* helps us cut the tree such that we achieve the desired number of cluster.

**Correlation Distance Graph**

```{r}
dd <- as.dist(1-cor(t(super[,-1])))  # find the correlation-based distance

kkc <- hclust(dd, method="complete")
plot(kkc, main="Complete Linkage with Correlation-Based Distance",
     xlab="", sub="")  
hc.cluster <- cutree(kkc, 3)
hc.cluster
table(km.cluster, hc.cluster)

kks <- hclust(dd, method="single")
plot(kks, main="Single Linkage with Correlation-Based Distance",
     xlab="", sub="")  
cutree(kks, 3)

kka <- hclust(dd, method="average")
plot(kka, main="Average Linkage with Correlation-Based Distance",
     xlab="", sub="")  
cutree(kka, 3)
```

**Euclidean Distance Graph**

```{r}
# Using Euclidean distance
ggc <- hclust(dist(super[,-1]), method="complete")  # use Euclidean distance
plot(ggc, main="Complete Linkage with Euclidean Distance",
     xlab="", sub="")
cutree(ggc, 3)

ggs <- hclust(dist(super[,-1]), method="single")
plot(ggs, main="Single Linkage with Euclidean Distance",
     xlab="", sub="")
cutree(ggs, 3)

gga <- hclust(dist(super[,-1]), method="average")
plot(gga, main="Average Linkage with Euclidean Distance",
     xlab="", sub="")
cutree(gga, 3)
```

### Considerations in Clustering

1.  Should observations first be standardized?
2.  How many clusters should be used for K-means clustering
3.  For hierarchical clustering, what dissimilarity measure, linkage and cutting point of the dendrogram should be used?

# Lesson 4: Multiple Linear Regression

## Basic theory of Regression Model
-   Interpretation of coefficient
-   Qualitative Variables 
-   Interaction variables
-   Non-linear effects
-   (Copy over from Stat learning)

## Dimension Reduction

### Best subset approach

-   (Copy over from Stat learning)

### Lasso / Ridge

-   (Copy over from Stat learning)

### PCR regression

#### Theory
-   PCR perform better: Lower variance but higher bias (When $M<p$)
-   When $M=p$ and no dimension reduction occurs, results are the same as OLS method.


#### Example 1

Used when there are correlated predictors in the model:

The code below does PCA and the regression separately
```{r}
library(pls)
blood <- read.csv("Datasets/BPressure.csv", stringsAsFactors = TRUE)
cor(blood)
reg.re <- lm(BP~., data=blood)
summary(reg.re)
predict(reg.re, blood)
pr.out <- prcomp(blood[,-1], scale=TRUE) # Removing the BP column
summary(pr.out)
pr.out$x
pr.out$rotation
lmpr <- lm(blood$BP~pr.out$x[,1]) # Using only the first principle component 
summary(lmpr)               
```
The code below uses a predefined function for PCR.
```{r}
pcr.re1 <- pcr(BP~., data=blood, scale=TRUE, ncomp=1)
summary(pcr.re1)
pcr.re1$fitted.values
newx1 <- pcr.re1$scores # Obtains the principal component score
newx1
newreg1 <- lm(blood$BP ~ newx1)
summary(newreg1)

# If you use M=p, then you go back to the original OLS method.
pcr.re2 <- pcr(BP~., data=blood, scale=TRUE, ncomp=2)
summary(pcr.re2)
pcr.re2$fitted.values
newx2 <- pcr.re2$scores
newx2
newreg2 <- lm(blood$BP ~ newx2)
summary(newreg2)
```

#### Example 1
```{r}
library(ISLR2)
library(pls)
names(Hitters)
dim(Hitters)
sum(is.na(Hitters$Salary))
Hitters <- na.omit(Hitters)
dim(Hitters)

# splitting train and test set
set.seed(1)
train <- sample(1:nrow(Hitters), nrow(Hitters)/2)
test <- (-train)

# run PCR on the training data with Cross Validation
set.seed(1)
pcr.fit <- pcr(Salary~., 
               data=Hitters, 
               subset=train,
               scale=TRUE, 
               validation="CV") # By default, it uses 10 fold validation.
summary(pcr.fit)
validationplot(pcr.fit, val.type="MSEP")
pcr.fit

# we find the lowest CV error when M=5
# Calculate the test MSE with five components
x <- model.matrix(Salary~., Hitters)[,-1]
y <- Hitters$Salary
pcr.pred <- predict(pcr.fit, x[test,], ncomp=5)
mean((pcr.pred-y[test])^2)

# fit PCR on the full data set with M=5
pcr.fitall <- pcr(Salary~., data=Hitters, scale=TRUE, ncomp=5)
summary(pcr.fitall)
```
#### Partial Least Squares Method
(To review theory again)
-   Difference to PCR:
    -    It identifies the new features in a supervised way:
        -   Take each variable of $X_1$ to regress onto each point of $Y_1$, to obtain the coefficient as $\hat{\beta}_{1,1} , \hat{\beta}_{1,2}, ...$
        -   Afterwhich take the residuals of the regression and do it again against $X_2$
        
```{r}
set.seed(1)
pls.fit <- plsr(Salary~., data=Hitters, subset=train,
                scale=TRUE, validation="CV") 
summary(pls.fit)
validationplot(pls.fit, val.type="MSEP")
# we find the lowest CV error when M=1
# Calculate the test MSE
pls.pred <- predict(pls.fit, x[test,], ncomp=1)
mean((pls.pred-y[test])^2)

# fit PLS on the full data set with M=1
pls.fitall <- plsr(Salary~., data=Hitters, scale=TRUE, ncomp=1)
summary(pls.fitall)
```



# Lesson 5: Classification

## Multinomial Logistic Regression

-   Theory to take from python notes

## Discriminant Analysis

### Theory (Bayes Theorem)

### Linear Discriminant Analysis (LDA)

### Quadratic Discriminant Analysis (QDA)

# Lesson 6: Support Vector Machines
