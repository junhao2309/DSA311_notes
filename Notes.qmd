---
title: "ML_Notes"
number-sections: true
format: 
  html:
    css: full-width.css
execute: 
  echo: true
  eval: true
  warning: false
editor: visual
toc: true
---

# Lesson 1: Overview of SL & ML

```{r, echo = FALSE}
library(knitr)

# Create a data frame
data <- data.frame(
  Areas = c("Scope", "Focus", "Data", "Approach","Concern", "Application"),
  Statistical_Learning = c("Subfield of Statistics", 
                           "Models building and their interpretability", "Use survey methods / experimental study to collect random data with a particular purpose or objective - understand ideas behind various techniques and accurately assess performance of each technique",
                           "Models with predefined assumptions and all data. Interpretable but holds limitations in capturing complex patterns",
                           "Parameter estimation and hypothesis testing at a certain error rate",
                           "Econometrics, biostatistics, finance, etc"),
  Machine_Learning = c("Subfield of AI", 
                       "Prediction accuracy", 
                       "Collect large or big data set in a routine way - Focuses on large scale applications",
                       "Use training data to build the model with no assumptions and use a test set to evaluation model",
                       "Bias-variance trade-off and prediction errors",
                       "Natural language proccessing, face recognition, traffic prediction, where predictive accuracy and pattern recognition are paramount")
)

# Create a table
kable(data, caption = "Statistical Learning VS Machine Learning")
```

```{r, echo = FALSE}
# Create a data frame
data <- data.frame(
  Areas = c("Focus", "Variables", "Purpose", "Concern"),
  Supervised_Learning = c("Focus on outcome measurement, Y (dependent variable)", 
                           "Use p predictor measurements (independent variables)", 
                          "For regression / classification problems",
                          "-")
                          ,
  Unsupervised_Learning = c("No outcome measurement, Y, just a set of predictors", 
                            "Find features of X that behave similarly or find linear combinations of X with the most variation", 
                            "Use unsupervised learning as first step of supervised learning",
                            "Difficult to evaluate the performance of approaches because of no outcome measurement for comparisons"
                       )
)

# Create a table
kable(data, caption = "Supervised learning VS Unsupervised learning")
```

## Some Basic Terminologies

-   Reducible Error $$
    f(X) - \hat{f}(X)
    $$ where $f(X)$ is the true value while $\hat{f}(x)$ is the predicted value

    -   We seek to find the most appropriate statistical learning technique that minimises the reducible error

-   Irreducible Error $$
    \epsilon = Y - f(X)
    $$

    -   $\epsilon$ cannot be predicted using $X$

-   Total Errors The average of the squared difference between the predicted $Y$ and actual value $Y$ is $$
    E(Y-\hat{Y})^2 = E[f(X) +\epsilon -\hat{f}(X)]^2 + Var(\epsilon)
    $$

$$
E(Y-\hat{Y})^2 = [f(X)-\hat{f}(X)]^2 + Var(\epsilon)
$$

-   Estimation of $f$

    -   Parametric
        -   Assumes a function form/shape of $f$
    -   Non-parametric
        -   Does not assume the shape of $f$

-   Training & Testing Data

    Objectives:

    -   Accurately predict unseen test cases
    -   Understand which independent variables affect the dependent variable, and how
    -   Assess quality of predictions or/and inferences

-   Prediction Accuracy VS Interpretability

    -   More restrictive (less flexibility) means more interpretable in inference objective
        -   Less flexibility, it is likely bias is higher. Vice Versa
        -   Less flexibility, it is likely variance is smaller. Vice Versa
    -   For prediction objective, less restrictive does not always yield the best model.

![](images/Lesson1_img1.png)

-   

    ```         
    Bias Variance Trade Off
    ```

    $$
    E(y_0-\hat{f}(x_0))^2 = Var(\hat{f}(x_0))+[Bias(\hat{f}(x_0))]^2+Var(\epsilon)
    $$

    -   Bias: Error that is introduced by approximating a real-life situation with a much simpler model $\hat{f}$
    -   Variance: Amount by which $\hat{f}$ would change if we estimated it again using a different training data set
    -   Relationship with flexibility: As flexibility increases, its variance increases while its bias decreases. ![](images/Lesson1_img2.png)

-   Validation

    -   Model is fitted on a training data set and tested with a validation set.
    -   The validation test set provides a validation-set error which provides an estimate of the test error

-   K-Fold Cross Validation

    -   Widely used approach for estimating test error
    -   Randomly divide the data into $K$ equal-sized parts
    -   Results from each K parts are combined at the end

-   Validation and Cross-Validation

    -   Use cross-validation to identify the parameter values for a given approach
    -   Use validation to choose best approach among all the considered approaches

As the foundation of this course is from Statistical learning with R, refer to [Statistical Learning Notes](file:///Users/junhaoteo/Documents/SMU_Modules/Y2S2/Statistical%20Learning%20with%20R/DSA211_notes.html) where the various codes can be obtained.

## Some basic codes:

-   Multiple regression model + Quadratic variables

```{r}
# Carseats example in textbook 
library(ISLR)     
attach(Carseats)
names(Carseats)
summary(Carseats)

#multiple regression model including all independent variables
lm.carseat1=lm(Sales~., data=Carseats)
summary(lm.carseat1)

# multiple regression model with significant factors
lm.carseat2=lm(Sales~CompPrice+Income+Advertising+Price+ShelveLoc+Age, 
               data=Carseats)
summary(lm.carseat2)

confint(lm.carseat2, level=0.95)
predict(lm.carseat2, 
        data.frame(CompPrice=116, Income=80, Advertising=7,
                                Price=100, ShelveLoc="Good", Age=56), 
        interval="confidence", 
        level=.95)
predict(lm.carseat2, 
        data.frame(CompPrice=116, Income=80, Advertising=7,
                                Price=100, ShelveLoc="Good", Age=56), 
        interval="prediction", 
        level=.95)

# quadratic relationship
lm.carseat3=lm(Sales~CompPrice+Income+Advertising+
                 Price+ShelveLoc+Age+I(Price^2), data=Carseats)
summary(lm.carseat3)
```

-   Code for train-test split for multiple regression

```{r}
library(ISLR)
attach(Auto)

set.seed(3344)
Auto1 <- Auto[,1:8]
train <- sample(1:nrow(Auto1), 200) #sample 200 out of the whole sample size
test <- -train
auto.train <- Auto1[train,]
auto.test <- Auto1[test,]

# multiple regression model
lm.fit <- lm(mpg~., data=auto.train)
lm.pred <- predict(lm.fit, auto.test)
mse.mrm <- mean((auto.test$mpg-lm.pred)^2)
mse.mrm
```

-   Lasso & Ridge Regularization

```{r}
#Lasso 
library(glmnet)
train.x <- model.matrix(mpg~., data=auto.train)[,-1]      # Remove the intercept
train.y <-auto.train$mpg
test.x <- model.matrix(mpg~., data=auto.test)[,-1]        # Remove the intercept
test.y <- auto.test$mpg

lasso.mod <- glmnet(train.x, train.y, alpha=1)
cv.out <- cv.glmnet(train.x, train.y, alpha=1,
                    nfolds = 10)            # Default 10 Fold, change this for other fold value
lambda.lasso <- cv.out$lambda.min
lambda.lasso
lasso.pred <- predict(lasso.mod, newx=test.x, s=lambda.lasso)
mse.lasso <- mean((test.y-lasso.pred)^2)
mse.lasso

#Ridge regression
ridge.mod <- glmnet(train.x, train.y, alpha=0)
cv.out <- cv.glmnet(train.x, train.y, alpha=0)
lambda.rr <- cv.out$lambda.min
lambda.rr
ridge.pred <- predict(ridge.mod, newx=test.x, s=lambda.rr)
mse.rr <- mean((test.y-ridge.pred)^2)
mse.rr
```

After comparing all models above using the Auto Dataset, we have that Lasso is the best model.

Rebuild the model using all of the data:

```{r}
x <- model.matrix(mpg~., data=Auto1)[,-1]
y <- Auto1$mpg
out.lasso <- glmnet(x,y, alpha=1)
lasso.m2 <- predict(out.lasso, type="coefficients", s=lambda.lasso)[1:8,]
lasso.m2[lasso.m2!=0]
```

## Classification Setting

-   Testing Error for Classification

-   Confusion Matrix

![](images/Lesson1_img3.png)

# Lesson 2: Unsupervised Learning I

## Principal Components Analysis (PCA)

-   What is it?

    -   A dimensionality reduction method by reducing the dimensionality of large data sets
    -   Finds a sequence of linear combinations of the variables that have **maximal variance**, and **mutually uncorrelated**

$$
X_1, X_2, â€¦, X_{1000} \\
\text{The above is converted through PCA to output: } \\
Var(Y_1+Y_2) = Var(Y_1)+Var(Y_2)+2Cov(Y_1, Y_2)
$$

```         
-   Reducing dimension comes at the expense of accuracy, but essentially to trade little accuracy for simplicity
    -   Low dimension data sets are easier to explore and visualize and makes analysing data points easier and faster for machine learning
```

-   Applications of PCA

Use the principal components:

```         
- In understanding and visualising the data
- As a tool for data imputation, for filling in missing values (No time to cover)
- As predictors in a regression model, instead of original larger set of independent variables
```

-   Mathematical Representation of PCA

Each observation (row) is multiplied to the loading vector to obtain the scores, $Z$.

Loading vector $$
\phi_i = (\phi_{11}, \phi_{21},... \phi_{p1})^T
$$

Under PCA, the objective is to minimise the perpendicular distance from the observation to the line.
As such the distance of the line is maximised.

Goal: $$
\max{\frac{\sum^{n}_{i=1} Z_i^2} {n}}
$$ (To input graphs from slides)

### Second Prinipal Component

Steps are similar to the First Principal Component, however, it has to be uncorrelated with $Z_1$.

### Singular Value Decomposition

(TO ADD IN THEORY)

#### Simple Numerical Example of Singular Value Decomposition - Manual calculation

```{r}
data <- c(3, 5, 5, 9, 1, 9, 3, 7, 4, 2, 5, 9, 1, 6, 8, 3)
A <- matrix(data, nrow = 4)
A

sA <- svd(A) # Do SVD
names(sA)

d <- sA$d
U <- sA$u
V <- sA$v
D <- diag(d)
```

```{r}
round(U, 2)
round(t(V), 2)
round(D, 2)
round(t(V) %*% V, 2) # Matrix multiplication - Becomes identity matrix
round(t(U) %*% U, 2) # Matrix multiplication - Becomes identity matrix
round(U %*% D %*% t(V), 2)
```

Using the example above, we have that:

![](images/Lesson2_img1.png)

```{r}
u1 <- U[,1]
u1
v1 <- V[,1]
v1
d1 <- d[1]
d1
A1 <- u1 %*% t(v1)*d1
round(A1,2)
```

We see that A1 is still quite far from the original A matrix.
By adding another principal component we have:

![](images/Lesson2_img2.png)

```{r}
u2 <- U[,2]
u2
v2 <- V[,2]
v2
d2 <- d[2]
d2
A2 <- u2 %*% t(v2)*d2
round(A2,2)
A1A2 <- A1+A2
round(A1A2,2)
```

From the above, we see that with PC1 and PC2, it gets closer to the original A matrix.

Repeating the steps one more time.

```{r}
u3 <- U[,3]
u3
v3 <- V[,3]
v3
d3 <- d[3]
d3
A3 <- u3 %*% t(v3)*d3
round(A3,2)
A1A2A3 <- A1+A2+A3
round(A1A2A3, 2)
```

## Proportion of Variance Explained (PVE)

-   Determines the number of principal components

    -   PVEs should sum to one when maximum number of principal component is used

### Coding

Using another numerical example:

```{r}
data <- c(12,13,15, 17, 21, 24, 26, 34, 28, 29, 43, 59, 20, 35, 30, 48, 32, 56)
M <- matrix(data, nrow=6)  # data matrix with dimension 6x3
M

# Manual calculation
X <- scale(M)  # scale the matrix M, to ensure data has a mean of 0
               # The standard deviation is also adjusted to 1
round(X, 4)
sX <- svd(X) # do SVD
names(sX)
d <- sX$d
U <- sX$u
V <- sX$v
D <- diag(d)
round(U, 4)
round(V, 4)
round(D, 4)
round(t(V) %*% V, 4)
round(t(U) %*% U, 4)
round(U %*% D %*% t(V), 4)
pcs <- U %*% D
pcs
pcss <- X %*% V
pcss
```

```{r}
# Using pcob
pcob <- prcomp(M, scale=TRUE)  # use R function prcomp, with scaling
                               # Use scaling when the features are very different.
names(pcob)
summary(pcob)
pcob$sdev      # standard deviation from PCs
pcob$rotation  # same as matrix V
pcob$center    # means for scaling
pcob$scale     # sd for scaling
pcob$x        # XV or UD
totalsum <- sum(pcss^2)  # total sum
pc1sum <- sum(pcss[,1]^2) # sum from PC1
pc2sum <- sum(pcss[,2]^2) # sum from PC2
pc3sum <- sum(pcss[,3]^2) # sum from PC3
sd <- sqrt(c(pc1sum, pc2sum, pc3sum)/(6-1)) # n-1=6-1=5 
sd
totalsum
pc1sum
pc2sum
pc3sum
propvar <- c(pc1sum, pc2sum, pc3sum)/totalsum
round(propvar, 4)
par(mfrow=c(1,2))
biplot(pcob, scale=0)
plot(pcob$x[,1], pcob$x[,2], xlab="PC1", ylab="PC2", ylim=c(-2, 2),
     main="Plot PC1 vs. PC2")
```

Using the US Arrest Dataset

```{r}
# Load the USArrests data and set row names as state names
data("USArrests")
states <- row.names(USArrests)
row.names(USArrests) <- states

# Display dimensions and variable names of the dataset
# dim(USArrests)
# names(USArrests)

# Optionally, calculate means and standard deviations for each variable
# apply(USArrests, 2, mean)
# apply(USArrests, 2, sd)

# Perform PCA on the scaled data
pr.out <- prcomp(USArrests, scale = TRUE)
summary(pr.out) # 2 principal components are sufficient

# Examine the components of the PCA output
pr.out$center  # Mean of each variable
pr.out$scale   # Scaling applied to each variable
pr.out$rotation  # The rotation (loading of each variable on the principal components)
```

```{r}
# Dimensions and plot of the first two principal components
dim(pr.out$x)
biplot(pr.out, scale = 0, main = "The first two Principal Components")
```

For the above graph, the top represents PC1 score while the right scale represents PC2 score.

```{r}
# To create a mirrored version of the biplot
## The software may result in different sign for its loading vectors, the rotation converts the sign.
pr.out$rotation <- -pr.out$rotation
pr.out$x <- -pr.out$x
biplot(pr.out, scale = 0, main = "The first two Principal Components mirror image")

# Calculate and print the proportion of variance explained by each principal component
pr.var <- pr.out$sdev^2
pve <- pr.var/sum(pr.var)
round(pve, 4)

# Plotting the variance explained
par(mfrow = c(1, 2))
plot(pve, xlab = "Principal Component", ylab = "Proportion of Variance Explained", ylim = c(0, 1), type = "b")
plot(cumsum(pve), xlab = "Cumulative Principal Component", ylab = "Proportion of Variance Explained", ylim = c(0, 1), type = "b")
```

# Lesson 3: Unsupervised Learning II

## Kmeans-clustering

An approach for partitioning a dataset into $K$ distinct, non-overlapping clusters - Properties

```         
- Each observation belongs to at least one of the K clusters
- Clusters are non-overlapping: No observation belongs to more than one cluster
```

-   Within Cluster Variation $$
    \min_{C_1,...,C_K} \{\sum^K_{k=1} WCV(C_k) \} 
    $$ The formula above is to be solved such that the total within-cluster variation, summed over all $K$ clusters, is minimised.

    -   Euclidean distance calculation: $$
        WCV(C_k)= \frac{1}{|C_k|} \sum_{i,i'\epsilon C_k} \sum^p_{j=1} (x_{ij}-x_{i'j})^2
        $$ where $|C_k|$ denotes the no. of obs in the $k$th cluster.

$(x_{ij}-x_{i'j})^2$ : Represents the distance between two observation

Combining the two formulas above gives us: $$
\min_{C_1,...,C_K} \{\sum^K_{k=1} \frac{1}{|C_k|} \sum_{i,i'\epsilon C_k} \sum^p_{j=1} (x_{ij}-x_{i'j})^2 \} 
$$ Can be simplied to: $$
2\sum_{i\subset C_k}\sum^{p}_{j=1} (x_{ij}-\bar{x}_{kj})^2
$$ \$\bar{x}\_{kj} \$ : Represents the mean of the cluster The code chunk below uses the formulas above to deduce the total sum.

(Need more inputs after lesson)

```{r}
d1 <- c(12, 13, 14, 17, 20)
d2 <- c(4, 7, 10)
d3 <- c(18, 19, 22, 25, 27)

su1=0
for (i in 1: length(d1)) {
  su1 <- su1 + sum((d1[i]-d1)^2)
}
s1 <- su1/length(d1)
s1

su2=0
for (i in 1: length(d2)) {
  su2 <- su2 + sum((d2[i]-d2)^2)
}
s2 <- su2/length(d2)
s2

su3=0
for (i in 1: length(d3)) {
  su3 <- su3 + sum((d3[i]-d3)^2)
}
s3 <- su3/length(d3)
s3

tots <- s1+s2+s3
tots

sum1 <- sum((d1-mean(d1))^2)
sum2 <- sum((d2-mean(d2))^2)
sum3 <- sum((d3-mean(d3))^2)
totsum <- 2*(sum1+sum2+sum3)
c(sum1, sum2, sum3, totsum)
```

### General steps in K-clustering

1.  Randomly assign a number, from 1 to $K$, to each observation.
    These serve as initial cluster assignments.

2.  Iterate until cluster assignments stop changing:

    2.1: For each of the $K$ clusters, compute the cluster centroid.
    The $k$th cluster centroid is the vector of the $p$ feature means for the observations in the $k$th cluster.

    2.2: Assign each observation to the cluster whose centroid is closest, defined by the Euclidean distance.

Visual Representation of the steps:

![](images/Lesson3_img3.png) \#### Example Code of the steps shown above

```{r}
set.seed(2)
x <- matrix(c(12, 13, 11, 18, 16, 17, 19, 20, 21, 22), ncol=2)
# x # To view the matrix
plot(x, main="plot of X1 and X2", xlab="X1", ylab="X2", pch=20, cex=2)
k.out <- kmeans(x, centers = 2, nstart=20)  # centers : No. of clusters, nstart: No. of random sets
k.out$cluster
plot(x, col=(k.out$cluster+1), main="K-means Clustering Results with K=2",
     xlab="X1", ylab="X2", pch=20, cex=2)
k.out
```

Total variation is reduced by 81.2% by clustering them.

Below is how the variation is manually calculated.
We can see that the end result is the same as the above

```{r}
# Manual calculation of the variation:

g1 <- k.out$centers[1,]
g2 <- k.out$centers[2,]
# calculate total sum of squares
tot <- rep(NA,5)
for (i in 1:5)
  tot[i] <- (x[i,1]-mean(x[,1]))^2+(x[i,2]-mean(x[,2]))^2
sstot <- sum(tot)
sstot
# calculate the sum of squares due to group 1
totg1 <- rep(NA,2)
for (i in 4:5)
  totg1[i-3] <- (x[i,1]-g1[1])^2+(x[i,2]-g1[2])^2
ssg1 <- sum(totg1)
ssg1
# calculate the sum of squares due to group 2
totg2 <- rep(NA,3)
for (i in 1:3)
  totg2[i] <- (x[i,1]-g2[1])^2+(x[i,2]-g2[2])^2
ssg2 <- sum(totg2)
ssg2

# calculate the proportion of sum of squares explained by clusters
1-(ssg1+ssg2)/sstot
```

### Practical Application

#### Initial Visualisation of PCA

```{r}
score <- read.csv("Datasets/Score.csv", stringsAsFactors = TRUE)
summary(score)
dim(score)
attach(score)
# perform PCA
pr.out <- prcomp(score)  # we do not scale the data  
summary(pr.out)
biplot(pr.out, scale=0)
pr.out$rotation
pc1 <- pr.out$x[,1]

plot(pc1, score$Participation)
plot(pc1, score$Assignment)
plot(pc1, score$Test)
plot(pc1, score$Exam)
```

#### K-means clustering

```{r}
set.seed(123)
k.out <- kmeans(score, 4, nstart=20)  # we set 4 clusters
k.out

# Only K-means
plot(score$Exam, score$Test, col=(k.out$cluster+1), 
     main="K-means Clustering Results with K=4 based on Exam and Test scores",
     xlab="Exam", ylab="Test", pch=20, cex=2)

# With PCA
plot(pr.out$x[,1], pr.out$x[,2], col=(k.out$cluster+1), 
     main="K-means Clustering Results with K=4 based on PCA",
     xlab="PC1", ylab="PC2", pch=20, cex=2)
```

## Hierarchical clustering - (To copy over from python notes)

### Theory

-   Difference to K-Means

    -   Does not require pre-determination of clusters
    -   Results in an attractive tree-based representation
    -   Dendrogram is built starting from the leaves and combining clusters up to the trunk (bottom-up clusters)

-   Methodology (As illustrated below):

    1.  Start with each point in its own cluster
    2.  Identify the closest two clusters and merge them
    3.  Repeat Step 1 and 2
    4.  Ends when all points are in a single cluster

![](images/Lesson3_img1.png)

For the example above,

1.  We start by identifying A and C to be the closest, forming the first part of the Dendrogram
2.  Afterwards, we find the next closest, which is D and E, which forms the second part of the Dendrogram.
3.  Next, cluster AC is closest to B and therefore we merge them together to form one cluster. This is represented in the Dendrogram as well.
4.  Lastly, cluster ACB and cluster DE are merged together, forming the entire Dendrogram

### Types of linkages and its numerical example

```{r, echo = FALSE}
# Create a data frame
data <- data.frame(
  Linkage = c("Complete", "Single", "Average"),
  Description = c("Maximal inter-cluster dissimilarity. Compute all pairwise dissimilarities between the observations in cluster A and the observations in cluster B, and record the largest of these dissimilarities.",
                  "Minimal inter-cluster dissimilarity. Compute all pairwise dissimilarities between the observations. Compute all pairwise dissimilarities between the observations in cluster A and the observations in cluster B, and record the smallest of these dissimilarities.",
                  "Mean inter-cluster dissimilarity. Compute all pairwise dissimilarities between the observations in cluster A and the observations in cluster B, and record the average of these dissimilarities.")
  
)

# Create a table
kable(data, caption = "Types of Linkages")
```

Formal formulas: Complete: $$
d_{CL}(G,H) =\max_{i\space \epsilon \space G,i' \space \epsilon \space H} {d_{ii'}}
$$ Let's say:

-   Cluster 1 has 2 observations: $$
    (2, 4), (3, 5)
    $$

-   Cluster 2 has 2 observations: $$
    (4, 6), (7,6)
    $$

-   Cluster 3 has 1 observation: $$
    (5, 9)
    $$

The inter-cluster dissimilarity between Clusters 1 and 2:

-   (2, 4) and (4, 6) dissimilarity measure =$\sqrt{(2-4)^2+(4-6)^2} = \sqrt{8}$

-   (2, 4) and (7, 6) dissimilarity measure =$\sqrt{(2-7)^2+(4-6)^2 }= \sqrt{29}$

-   (3, 5) and (4, 6) dissimilarity measure =$\sqrt{(3-4)^2+(5-6)^2}=\sqrt{2}$

-   (3, 5) and (7, 6) dissimilarity measure =$\sqrt{(3-7)^2+(5-6)^2}=\sqrt{17}$

The inter-cluster dissimilarity between Clusters 2 and 3:

-   (4, 6) and (5, 9) dissimilarity measure =$\sqrt{(4-5)^2+(6-9)^2} = \sqrt{10}$

-   (7, 6) and (5, 9) dissimilarity measure =$\sqrt{(7-5)^2+(6-9)^2 }= \sqrt{13}$

The inter-cluster dissimilarity between Clusters 1 and 3:

-   (2, 4) and (5, 9) dissimilarity measure =$\sqrt{(2-5)^2+(4-9)^2} = \sqrt{34}$

-   (3, 5) and (5, 9) dissimilarity measure =$\sqrt{(3-5)^2+(5-9)^2 }= \sqrt{20}$

**Complete linkage**: Focus on farthest pair of points, producing tight, compact clusters

Therefore, inter-cluster dissimilarity between Clusters 1 and 2 = $\max(\sqrt{8},\sqrt{29},\sqrt{2},\sqrt{17})=\sqrt{29}$

**Single linkage**: Focuses on closest pair of points, resulting in elongated clusters

Therefore, inter-cluster dissimilarity between Clusters 1 and 2 = $\min(\sqrt{8},\sqrt{29},\sqrt{2},\sqrt{17})=\sqrt{2}$

**Average linkage**: Leads to balanced clustering

Therefore, inter-cluster dissimilarity between Clusters 1 and 2 = $\frac{(\sqrt{8}+\sqrt{29}+\sqrt{2}+\sqrt{17})}{4}=3.4377$

### Additional information:

-   

    ```         
    Correlation-based distance
    ```

    -   If two observations are highly correlated but they have high Euclidean distance, correlation-based distance may be prefered
        -   This measure focuses on the shapes of observation profiles rather than their magnitudes

![](images/Lesson3_img2.png)

### Example Code

```{r}
super <- read.csv("Datasets/Supermarket.csv", stringsAsFactors = TRUE)
summary(super)
# View(super)

set.seed(9876)
k.out <- kmeans(super[,-1], 3, nstart=20) # Remove the customer column
k.out
km.cluster <- k.out$cluster
```

Note that under Dendrogram, the function *cutree(tree, cluster)* helps us cut the tree such that we achieve the desired number of cluster.

**Correlation Distance Graph**

```{r}
dd <- as.dist(1-cor(t(super[,-1])))  # find the correlation-based distance

kkc <- hclust(dd, method="complete")
plot(kkc, main="Complete Linkage with Correlation-Based Distance",
     xlab="", sub="")  
hc.cluster <- cutree(kkc, 3)
hc.cluster
table(km.cluster, hc.cluster)

kks <- hclust(dd, method="single")
plot(kks, main="Single Linkage with Correlation-Based Distance",
     xlab="", sub="")  
cutree(kks, 3)

kka <- hclust(dd, method="average")
plot(kka, main="Average Linkage with Correlation-Based Distance",
     xlab="", sub="")  
cutree(kka, 3)
```

**Euclidean Distance Graph**

```{r}
# Using Euclidean distance
ggc <- hclust(dist(super[,-1]), method="complete")  # use Euclidean distance
plot(ggc, main="Complete Linkage with Euclidean Distance",
     xlab="", sub="")
cutree(ggc, 3)

ggs <- hclust(dist(super[,-1]), method="single")
plot(ggs, main="Single Linkage with Euclidean Distance",
     xlab="", sub="")
cutree(ggs, 3)

gga <- hclust(dist(super[,-1]), method="average")
plot(gga, main="Average Linkage with Euclidean Distance",
     xlab="", sub="")
cutree(gga, 3)
```

### Considerations in Clustering

1.  Should observations first be standardized?
2.  How many clusters should be used for K-means clustering
3.  For hierarchical clustering, what dissimilarity measure, linkage and cutting point of the dendrogram should be used?

# Lesson 4: Multiple Linear Regression

## Basic theory of Regression Model

-   Interpretation of coefficient
-   Qualitative Variables
-   Interaction variables
-   Non-linear effects
-   (Copy over from Stat learning)

## Dimension Reduction

### Best subset approach

-   (Copy over from Stat learning)

### Lasso / Ridge

-   (Copy over from Stat learning)

### PCR regression

#### Theory

-   PCR perform better: Lower variance but higher bias (When $M<p$)
-   When $M=p$ and no dimension reduction occurs, results are the same as OLS method.

#### Example 1

Used when there are correlated predictors in the model:

The code below does PCA and the regression separately

```{r}
library(pls)
blood <- read.csv("Datasets/BPressure.csv", stringsAsFactors = TRUE)
cor(blood)
reg.re <- lm(BP~., data=blood)
summary(reg.re)
predict(reg.re, blood)
pr.out <- prcomp(blood[,-1], scale=TRUE) # Removing the BP column
summary(pr.out)
pr.out$x
pr.out$rotation
lmpr <- lm(blood$BP~pr.out$x[,1]) # Using only the first principle component 
summary(lmpr)               
```

The code below uses a predefined function for PCR.

```{r}
pcr.re1 <- pcr(BP~., data=blood, scale=TRUE, ncomp=1)
summary(pcr.re1)
pcr.re1$fitted.values
newx1 <- pcr.re1$scores # Obtains the principal component score
newx1
newreg1 <- lm(blood$BP ~ newx1)
summary(newreg1)

# If you use M=p, then you go back to the original OLS method.
pcr.re2 <- pcr(BP~., data=blood, scale=TRUE, ncomp=2)
summary(pcr.re2)
pcr.re2$fitted.values
newx2 <- pcr.re2$scores
newx2
newreg2 <- lm(blood$BP ~ newx2)
summary(newreg2)
```

#### Example 1

```{r}
library(ISLR2)
library(pls)
names(Hitters)
dim(Hitters)
sum(is.na(Hitters$Salary))
Hitters <- na.omit(Hitters)
dim(Hitters)

# splitting train and test set
set.seed(1)
train <- sample(1:nrow(Hitters), nrow(Hitters)/2)
test <- (-train)

# run PCR on the training data with Cross Validation
set.seed(1)
pcr.fit <- pcr(Salary~., 
               data=Hitters, 
               subset=train,
               scale=TRUE, 
               validation="CV") # By default, it uses 10 fold validation.
summary(pcr.fit)
validationplot(pcr.fit, val.type="MSEP")
pcr.fit

# we find the lowest CV error when M=5
# Calculate the test MSE with five components
x <- model.matrix(Salary~., Hitters)[,-1]
y <- Hitters$Salary
pcr.pred <- predict(pcr.fit, x[test,], ncomp=5)
mean((pcr.pred-y[test])^2)

# fit PCR on the full data set with M=5
pcr.fitall <- pcr(Salary~., data=Hitters, scale=TRUE, ncomp=5)
summary(pcr.fitall)
```

#### Partial Least Squares Method

(To review theory again) - Difference to PCR: - It identifies the new features in a supervised way: - Take each variable of $X_1$ to regress onto each point of $Y_1$, to obtain the coefficient as $\hat{\beta}_{1,1} , \hat{\beta}_{1,2}, ...$ - Afterwhich take the residuals of the regression and do it again against $X_2$

```{r}
set.seed(1)
pls.fit <- plsr(Salary~., data=Hitters, subset=train,
                scale=TRUE, validation="CV") 
summary(pls.fit)
validationplot(pls.fit, val.type="MSEP")
# we find the lowest CV error when M=1
# Calculate the test MSE
pls.pred <- predict(pls.fit, x[test,], ncomp=1)
mean((pls.pred-y[test])^2)

# fit PLS on the full data set with M=1
pls.fitall <- plsr(Salary~., data=Hitters, scale=TRUE, ncomp=1)
summary(pls.fitall)
```

# Lesson 5: Classification

## Multinomial Logistic Regression

-   Theory to take from python notes

![](images/Lesson5_img1.png) \## Linear and Quadratic Discriminant Analysis

### Theory (Bayes Theorem)

Base on three metrics: $\mu_1 , \mu_2 , \mu_3$ and \$\sum\_i \$

For each value of $Y$, we take the mean,$\mu_i$ of the $X$ features and its $\sum_i$

-   Manual mathematical calculation

```{r}
mlr <- read.csv("Datasets/MLR.csv", stringsAsFactors = TRUE)
library(nnet)
library(broom)
summary(mlr)
fit.mlr <- multinom(Y~., data=mlr)
summary(fit.mlr)
tidy(fit.mlr, conf.int = TRUE)
round(fit.mlr$fitted.values,3)
model <- predict(fit.mlr, mlr)
model
mlr$Y
table(model, mlr$Y)
# calculate the predicted prob for 5th obs.
bbcoef <- c(-6.733, 0.2555, -1.052, 1.1302)
cccoef <- c(-1.218, 0.1759, -0.6155, 0.3546)
fifthobs <- c(1, 28, 6, 7)
den <- 1+exp(sum(bbcoef*fifthobs))+exp(sum(cccoef*fifthobs))
proaa <- 1/den
probb <- exp(sum(bbcoef*fifthobs))/den
procc <- exp(sum(cccoef*fifthobs))/den
proaa
probb
procc
```

```{r, eval = FALSE}
library(ISLR2)
library(MASS) 
View(Smarket)
summary(Smarket)
dim(Smarket)
train <- (Smarket$Year <2005)
SMtrain <- Smarket[train,]
SMtest <- Smarket[!train,]
dim(SMtest)
y.test <- SMtest$Direction
lda.fit <- lda(Direction ~Lag1 +Lag2, data=Smarket, subset=train)
lda.fit
plot(lda.fit)
# verify the computer outputs
up <- SMtrain$Direction=="Up"
probup <- length(SMtrain$Direction[up])/length(SMtrain$Direction)
probup
mean(SMtrain$Lag1[!up])
mean(SMtrain$Lag1[up])
mean(SMtrain$Lag2[!up])
mean(SMtrain$Lag2[up])
lda.pred <- predict(lda.fit, SMtest)
lda.class <- lda.pred$class
table(lda.class, y.test)
lda.pred$posterior
#run the QDA
qda.fit <- qda(Direction ~Lag1 +Lag2, data=Smarket, subset=train)
qda.fit
qda.pred <- predict(qda.fit, SMtest)
qda.class <- qda.pred$class
table(qda.class, y.test)
qda.pred$posterior 
```

### Linear Discriminant Analysis (LDA)

### Quadratic Discriminant Analysis (QDA)

# Lesson 6: Support Vector Machines

A hyperplane has the form: $$
\beta_0+\beta_1X_1+\beta_2X_2 +... +\beta_pX_p=0
$$ - Has a subspace of dimension $p-1$ where $p$ is the number of features

![](images/Lesson5_img2.png) The above graph showcases a one dimensional plane.
A vertical line (hyperplane) can be drawn to separate the blue and red dots.

-   Ideally the distance from the blue dot to the line and the distance from the red dot to the line should be approximately equal. (See topic on margin)

![](images/Lesson5_img3.png) Example: The hyperplane separates the points: - For the blue line, any point on the right is positive while any point on the left is negative $$
-6+0.8X_1+0.6X_2=0
$$ Looking at Points (7,4) and (2,5), Sub in the points into the above equation

```{r}
-6+0.8*(7)+0.6*4 # Positive
-6+0.8*(2)+0.6*5 # negative
```

## Separating Hyperplane

Insert slide 6 of Note 6

Ensure that all purple points are more than 0.
Any deviations would suggest misclassification aka, the hyperplane did not properly separate the two $Y$ observations.

## Margin

-   Denoted as $M$
    -   Distance from the observation to the hyperplane is known as Margin

## Maximal Margin Classifier

-   Maximise the margin (biggest gap) between the two classes

Insert slide 8

The points that results in a distance within the margin is not that important.

## Support Vector classifier

More often, data points generally do not allow for a clean separation.
This is where we use support vector classifier that maximises a soft margin.

Insert Slide 11: - This allows for some misclassification that maximises the margin.
- If a hard margin is wanted, one will have to sacrifise for a smaller margin.

Insert slide 12

Point 1 is called the incorrect side of the margin as it passed the margin line.

Point 11 is called the incorrect side of the hyperplane.

Insert slide 13:

The parameter $C$ controls for the severity of the classication/margin errors.

Insert Slide 14:

From the graph above: $\epsilon_i=0$: 2, 3, 4, 5, 6,7, 9, 10 $0<\epsilon_i<1$: 1, 8 $\epsilon_i>1$: 11, 12

Support vectors are: 2, 7, 9

Insert Slide 16:

Why variance is reduced when C is bigger?
Since the margin is much wider, this means more observations becomes the support vector.
If any one of the observation changes, it has little impact on the hyperplane as compared to when there is only two support vector.

**CODING FOR SVM**

The key is to maximise $M$ subject to $\sum^{p}_{j=1} \beta_j^2=1$

However, in SVM in R, it only takes in the "cost" argument.

-   A larger cost parameter $\rightarrow$ Low Budget C
    -   Narrower margin
    -   Fewer support vector
    -   Lower bias and higher variance
-   A smaller cost parameter $\rightarrow$ High Budget C
    -   Larger margin
    -   More support vector
    -   Higher bias and lower variance

## Numeric Example 1:

### Simulation of data and basic logistic regression

```{r}
#Support Vector Classifier example
# numerical example 1

# Simulation of Data
y <- c(rep("aa", 5), rep("bb", 7))
x1<- c(2,1,7,6,4,6,3,5,8,6,5,9)
x2 <-c(5,3,4,1,2,8,4,9,2,6,7,3)
ss<- c(rep(1,5), rep(2,7))
plot(x2,x1, col=ss+1, cex=1.5)
dd3 <- data.frame(y, x1, x2, stringsAsFactors = TRUE)

# Simple logistics regression
log1 <- glm(y~x1+x2, data=dd3, family=binomial)
log1
predict(log1, data=dd3, type="response")
gg <- rep("predict aa", 12)
gg[log1$fitted.values>0.5] <- "Predicted bb"
table(gg, y)
gg
```

### SVM with cost= 0.10

```{r}
library(e1071)
svm1 <- svm(y~x1+x2, data=dd3, kernel="linear", cost=.1, scale=FALSE)
plot(svm1,dd3)
svm1
svm1$index
svm1$fitted
```

### SVM with cost= 10

```{r}
svm2 <- svm(y~x1+x2, data=dd3, kernel="linear", cost=10, scale=FALSE)
plot(svm2,dd3)
svm2
svm2$index
svm2$fitted
```

## Classification with Non-Linear Decision Boundaries (Numeric Example 2)

Purpose: To address **non-linearity** in the model - Enlarge Features with Polynomial Functions - Enlarge the feature space by including quadratic/cubic polynomials.

-   Inner Product: Takes the sum of point coordinates $$
    (x_i, x_i')=\sum^{p}_{i=1} x_{ij}x_{i'j}
    $$ For example: $X_1: 4, 7, 6, 9$ $X_2: 2, 5, 4, 6$

$(x_i, x_i')= 4\times2+7\times5+6\times4+9\times6$

$$
f(x)=\beta_0 +\sum^{n}_{i=1} \alpha_i(x,x_i)
$$ Most $\alpha$s can be zero as the observation that are away from the separation line do not contribute much to the parameters.

-   Linear Kernel
    -   Quantifies the similarity of the two observations

## Numeric Example 2:

### Simulation of data

```{r}
# numerical example 2:
library(e1071)
set.seed(1)
x <- matrix(rnorm(20*2), ncol=2)
y <- c(rep(-1, 10), rep(1, 10))
x[y == 1,] <- x[y ==1, ]+1
plot(x[,2], x[,1], col=3-y)
dat <- data.frame(x=x, y=as.factor(y))
dat
```

### SVM with cost 10

```{r}
svmfit <- svm(y ~ ., data =dat, kernel="linear", cost =10, scale=FALSE)
plot(svmfit, dat)
svmfit$fitted # Shows how SVM classifies the points
svmfit$index
summary(svmfit)
table(svmfit$fitted, y)
```

### SVM with cost 0.1

```{r}
svmfit1 <- svm(y ~ ., data =dat, kernel="linear", cost =0.1, scale=FALSE)
plot(svmfit1, dat)
svmfit$fitted # Shows how SVM classifies the points
svmfit1$index
summary(svmfit1)
table(svmfit1$fitted, y)
```

### Tuning (Cross validation)

```{r}
set.seed(1)
tune.out <- tune(svm, y~., data=dat, kernel="linear", 
                 ranges=list(cost=c(0.001, 0.01, 0.1, 1, 5, 10, 100))) # Testing a range of cost
summary(tune.out)
bestmod <- tune.out$best.model


set.seed(1)
xtest <- matrix(rnorm(20*2), ncol=2)
ytest <- sample(c(-1, 1), 20 , rep=TRUE)
xtest[ytest==1, ] <- xtest[ytest==1,]+1
testdat <- data.frame(x=xtest, y=as.factor(ytest))
ypred <- predict(bestmod, testdat)
table(predict=ypred, truth=testdat$y)
```

## Polynomial Kernel

$$
f(x)=\beta_0+\sum_{i\subset S} \alpha_i K(x,x_i)
$$

$$
K(x_i, x_i')=(1+\sum^p_{j=1} x_{ij}x_{i'j})^d
$$

-   d=1, it is standard linear kernel
-   d\>1, it fits a SVC in a higher dimensional space.

## Radial Kernel

$$
K(x_i,x_{i'})=exp(-\gamma \sum^p_{j=1} (x_{ij}-x_{i'j})^2)
$$

$$
f(x)=\beta_0 +\sum_{i\subset S} \hat{\alpha}_iK(x,x_i)
$$ This Kernel is more concern about the squared distance between two observation.
- Thereby controls variance by reducing most dimensions severely

Insert Slide 30 Graph

Insert Slide 31: R function svm() will perform multi-class classification using OVO.

## Numeric Example 3:

### Simulation of data

```{r}
# Support Vector Machine
# numerical example 3: radial kernel
set.seed(1)
x <- matrix(rnorm(200*2), ncol=2)
x[1:100,] <- x[1:100, ]+2
x[101:150, ] <- x[101:150,]-2
y <- c(rep(1, 150), rep(2, 50))
dat <- data.frame(x=x, y=as.factor(y))
dat
plot(x[,2], x[,1], col=y)
```

### Train-test split

```{r}
train <- sample(200, 100)
svmfit3 <- svm(y~., data=dat[train,], kernel="radial",
               gamma=1, cost=1)
plot(svmfit3, dat[train,])
```

### Perform CV to select $\gamma$ and cost

```{r}
#perform the cross-validation to select gamma and cost
tune.out <- tune(svm, y~., data=dat[train,],
                 kernel ="radial",
                 ranges=list(
                   cost=c(0.1,1,10,100, 1000),
                   gamma=c(0.5, 1, 2, 3, 4)))
summary(tune.out)
tune.out$best.parameters
bestmod1 <- tune.out$best.model
pred1 <- predict(bestmod1, newdata=dat[-train,])
table(true=dat[-train, "y"], pred1)
```
