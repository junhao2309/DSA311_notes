---
title: "ML_Notes"
number-sections: false
format: 
  html:
    css: full-width.css
execute: 
  echo: true
  eval: true
  warning: false
editor: visual
toc: true
---

```{r, echo=FALSE}
library(emoji)
```

# **Lesson 1: Overview of SL & ML** `r emoji::emoji("cats")`

```{r, echo = FALSE}
library(knitr)

# Create a data frame
data <- data.frame(
  Areas = c("Scope", "Focus", "Data", "Approach","Concern", "Application"),
  Statistical_Learning = c("Subfield of Statistics", 
                           "Models building and their interpretability", "Use survey methods / experimental study to collect random data with a particular purpose or objective - understand ideas behind various techniques and accurately assess performance of each technique",
                           "Models with predefined assumptions and all data. Interpretable but holds limitations in capturing complex patterns",
                           "Parameter estimation and hypothesis testing at a certain error rate",
                           "Econometrics, biostatistics, finance, etc"),
  Machine_Learning = c("Subfield of AI", 
                       "Prediction accuracy", 
                       "Collect large or big data set in a routine way - Focuses on large scale applications",
                       "Use training data to build the model with no assumptions and use a test set to evaluation model",
                       "Bias-variance trade-off and prediction errors",
                       "Natural language proccessing, face recognition, traffic prediction, where predictive accuracy and pattern recognition are paramount")
)

# Create a table
kable(data, caption = "Statistical Learning VS Machine Learning")
```

```{r, echo = FALSE}
# Create a data frame
data <- data.frame(
  Areas = c("Focus", "Variables", "Purpose", "Concern"),
  Supervised_Learning = c("Focus on outcome measurement, Y (dependent variable)", 
                           "Use p predictor measurements (independent variables)", 
                          "For regression / classification problems",
                          "-")
                          ,
  Unsupervised_Learning = c("No outcome measurement, Y, just a set of predictors", 
                            "Find features of X that behave similarly or find linear combinations of X with the most variation", 
                            "Use unsupervised learning as first step of supervised learning",
                            "Difficult to evaluate the performance of approaches because of no outcome measurement for comparisons"
                       )
)

# Create a table
kable(data, caption = "Supervised learning VS Unsupervised learning")
```

## Some Basic Terminologies

-   Reducible Error $$
    f(X) - \hat{f}(X)
    $$ where $f(X)$ is the true value while $\hat{f}(x)$ is the predicted value

    -   We seek to find the most appropriate statistical learning technique that minimises the reducible error

-   Irreducible Error $$
    \epsilon = Y - f(X)
    $$

    -   $\epsilon$ cannot be predicted using $X$

-   Total Errors The average of the squared difference between the predicted $Y$ and actual value $Y$ is $$
    E(Y-\hat{Y})^2 = E[f(X) +\epsilon -\hat{f}(X)]^2 + Var(\epsilon)
    $$

$$
E(Y-\hat{Y})^2 = [f(X)-\hat{f}(X)]^2 + Var(\epsilon)
$$

-   Estimation of $f$

    -   Parametric (Linear, logistic, regression tree, Lasso)
        -   Assumes a function form/shape of $f$
        -   Use the training data to fit
    -   Non-parametric (Random forest, bagging, bootstrap)
        -   Does not assume the shape of $f$
        -   Need a very large number of observations

-   Training & Testing Data

    Objectives:

    -   Accurately predict unseen test cases
    -   Understand which independent variables affect the dependent variable, and how
    -   Assess quality of predictions or/and inferences

-   Prediction Accuracy VS Interpretability

    -   More restrictive (less flexibility) means more interpretable in inference objective
        -   Less flexibility, it is likely bias is higher and variance is smaller. Vice Versa
    -   For prediction objective, less restrictive does not always yield the best model.

![](images/Lesson1_img1.png)

-   Bias Variance Trade Off

    $$
    E(y_0-\hat{f}(x_0))^2 = Var(\hat{f}(x_0))+[Bias(\hat{f}(x_0))]^2+Var(\epsilon)
    $$

    -   Bias: Error that is introduced by approximating a real-life situation with a much simpler model $\hat{f}$
    -   Variance: Amount by which $\hat{f}$ would change if we estimated it again using a different training data set
    -   Relationship with flexibility: As flexibility increases, its variance increases while its bias decreases. ![](images/Lesson1_img2.png)

-   Validation

    -   Model is fitted on a training data set and tested with a validation set.
    -   The validation test set provides a validation-set error which provides an estimate of the test error

-   K-Fold Cross Validation

    -   Widely used approach for estimating test error
    -   Randomly divide the data into $K$ equal-sized parts
    -   Results from each K parts are combined at the end

-   Validation and Cross-Validation

    -   Use cross-validation to identify the parameter values for a given approach
    -   Use validation to choose best approach among all the considered approaches

As the foundation of this course is from Statistical learning with R, refer to [Statistical Learning Notes](file:///Users/junhaoteo/Documents/SMU_Modules/Y2S2/Statistical%20Learning%20with%20R/DSA211_notes.html) where the various codes can be obtained.

## Some basic codes:

-   Multiple regression model + Quadratic variables

```{r}
# Carseats example in textbook 
library(ISLR)     
attach(Carseats)
names(Carseats)
summary(Carseats)

#multiple regression model including all independent variables
lm.carseat1=lm(Sales~., data=Carseats)
summary(lm.carseat1)

# multiple regression model with significant factors
lm.carseat2=lm(Sales~CompPrice+Income+Advertising+Price+ShelveLoc+Age, 
               data=Carseats)
summary(lm.carseat2)

confint(lm.carseat2, level=0.95)
predict(lm.carseat2, 
        data.frame(CompPrice=116, Income=80, Advertising=7,
                                Price=100, ShelveLoc="Good", Age=56), 
        interval="confidence", 
        level=.95)
predict(lm.carseat2, 
        data.frame(CompPrice=116, Income=80, Advertising=7,
                                Price=100, ShelveLoc="Good", Age=56), 
        interval="prediction", 
        level=.95)

# quadratic relationship
lm.carseat3=lm(Sales~CompPrice+Income+Advertising+
                 Price+ShelveLoc+Age+I(Price^2), data=Carseats)
summary(lm.carseat3)
```

-   Code for train-test split for multiple regression

```{r}
library(ISLR)
attach(Auto)

set.seed(3344)
Auto1 <- Auto[,1:8]
train <- sample(1:nrow(Auto1), 200) #sample 200 out of the whole sample size
test <- -train
auto.train <- Auto1[train,]
auto.test <- Auto1[test,]

# multiple regression model
lm.fit <- lm(mpg~., data=auto.train)
lm.pred <- predict(lm.fit, auto.test)
mse.mrm <- mean((auto.test$mpg-lm.pred)^2)
mse.mrm
```

-   Lasso & Ridge Regularization

```{r}
#Lasso 
library(glmnet)
train.x <- model.matrix(mpg~., data=auto.train)[,-1]      # Remove the intercept
train.y <-auto.train$mpg
test.x <- model.matrix(mpg~., data=auto.test)[,-1]        # Remove the intercept
test.y <- auto.test$mpg

lasso.mod <- glmnet(train.x, train.y, alpha=1)
cv.out <- cv.glmnet(train.x, train.y, alpha=1,
                    nfolds = 10)            # Default 10 Fold, change this for other fold value
lambda.lasso <- cv.out$lambda.min
lambda.lasso
lasso.pred <- predict(lasso.mod, newx=test.x, s=lambda.lasso)
mse.lasso <- mean((test.y-lasso.pred)^2)
mse.lasso

#Ridge regression
ridge.mod <- glmnet(train.x, train.y, alpha=0)
cv.out <- cv.glmnet(train.x, train.y, alpha=0)
lambda.rr <- cv.out$lambda.min
lambda.rr
ridge.pred <- predict(ridge.mod, newx=test.x, s=lambda.rr)
mse.rr <- mean((test.y-ridge.pred)^2)
mse.rr
```

After comparing all models above using the Auto Dataset, we have that Lasso is the best model.

Rebuild the model using all of the data:

```{r}
x <- model.matrix(mpg~., data=Auto1)[,-1]
y <- Auto1$mpg
out.lasso <- glmnet(x,y, alpha=1)
lasso.m2 <- predict(out.lasso, type="coefficients", s=lambda.lasso)[1:8,]
lasso.m2[lasso.m2!=0]
```

## Classification Setting

-   Testing Error for Classification

    -   False negative rate (FNR): Incorrectly assign a default individual to the no default category
    -   False positive rate (FPR): Incorrectly assign a no default individual to the default category

-   Confusion Matrix

    -   Sensitivity: % of true defaulters that are identified as defaulters
    -   Specificity: % of true non-defaulters that are correctly identified as non-defaulters

Formula:\
$$
Sensitivity = \frac{Predicted \space Default}{Total \space no.\space of\space True \space Default}
$$ $$
Specificity = \frac{Predicted \space No \space Default}{Total \space no.\space of\space True \space No \space Default}
$$ ![](images/Lesson1_img3.png)

# **Lesson 2: Unsupervised Learning I** `r emoji::emoji("dogs")`

-   Goals of Unsupervised learning

    -   Observe only the features, $X_1, X_2,..., X_p$
    -   Gather insights from the data and is not interested in prediction

## Principal Components Analysis (PCA)

-   What is it?

    -   A dimensionality reduction method by reducing the dimensionality of large data sets
    -   Finds a sequence of linear combinations of the variables that have **maximal variance**, and **mutually uncorrelated**
    -   Reducing dimension comes at the expense of accuracy, but essentially to trade little accuracy for simplicity
    -   Low dimension data sets are easier to explore and visualize and makes analysing data points easier and faster for machine learning

Below refers to the above point 2: $$
X_1, X_2, â€¦, X_{1000}
$$

$$
\text{The above is converted through PCA to output: } 
$$

$$
Var(Y_1+Y_2) = Var(Y_1)+Var(Y_2)+2Cov(Y_1, Y_2)
$$

**Applications of PCA**

Use the principal components:

-   In understanding and visualising the data

-   As a tool for data imputation, for filling in missing values (No time to cover)

-   As predictors in a regression model, instead of original larger set of independent variables

Mathematical Representation of PCA

Each observation (X_1,X_2,...,X_p) is multiplied to the loading vector to obtain the scores, $Z$.

Observations vector: $$
X =
\begin{bmatrix}
  X_1 & X_2 & ... & X_p
\end{bmatrix}
$$

Loading vector: \$\$ \phi\_i =

```{=tex}
\begin{bmatrix}
\phi_{11} \\ \phi_{21} \\... \\ \phi_{p1}

\end{bmatrix}
```
\$\$ The above creates a normalized linear combination of features:

\$\$ Z=X\times\phi\_i=

```{=tex}
\begin{bmatrix}
  X_1 & X_2 & ... & X_p
\end{bmatrix} \begin{bmatrix}
\phi_{11} \\ \phi_{21} \\... \\ \phi_{p1}

\end{bmatrix}
```
\\

Z= \phi\*{11}X_1+\*\phi{21}X_2+...+\phi\_{p1}X_p

\$\$

Under PCA, the objective is to minimise the perpendicular distance from the observation to the line.
As such the distance of the line is maximised.

Goal:

$$
\max{\frac{\sum^{n}_{i=1} Z_i^2} {n}}
$$

### Second Prinipal Component

Steps are similar to the First Principal Component, however, it has to be uncorrelated with $Z_1$.

### Singular Value Decomposition

(TO ADD IN THEORY)

#### Simple Numerical Example of Singular Value Decomposition - Manual calculation

```{r}
data <- c(3, 5, 5, 9, 1, 9, 3, 7, 4, 2, 5, 9, 1, 6, 8, 3)
A <- matrix(data, nrow = 4)
A

sA <- svd(A) # Do SVD
names(sA)

d <- sA$d
U <- sA$u
V <- sA$v
D <- diag(d)
```

```{r}
round(U, 2)
round(t(V), 2)
round(D, 2)
round(t(V) %*% V, 2) # Matrix multiplication - Becomes identity matrix
round(t(U) %*% U, 2) # Matrix multiplication - Becomes identity matrix
round(U %*% D %*% t(V), 2)
```

Using the example above, we have that:

![](images/Lesson2_img1.png)

```{r}
# Third principal component set of values
u1 <- U[,1]
v1 <- V[,1]
d1 <- d[1]
A1 <- u1 %*% t(v1)*d1
```

```{r}
u1
v1
d1
A1
round(A1,2)
```

We see that A1 is still quite far from the original A matrix.
By adding another principal component we have:

![](images/Lesson2_img2.png)

```{r}
# Second principal component set of values
u2 <- U[,2]
v2 <- V[,2]
d2 <- d[2]
A2 <- u2 %*% t(v2)*d2
A1A2 <- A1+A2
```

```{r}
u2
v2
d2
A2
round(A2,2)
round(A1A2, 2)
```

From the above, we see that with PC1 and PC2, it gets closer to the original A matrix.

Repeating the steps one more time.

![](images/Lesson2_img3.png)

```{r}
# Third principal component set of values
u3 <- U[,3]
v3 <- V[,3]
d3 <- d[3]
A3 <- u3 %*% t(v3)*d3
A1A2A3 <- A1+A2+A3
```

```{r}
u3
v3
d3
A3
round(A3,2)
round(A1A2A3, 2)
```

## Proportion of Variance Explained (PVE)

-   Determines the number of principal components

    -   PVEs should sum to one when maximum number of principal component is used

### Coding

Using another numerical example:

```{r}
data <- c(12,13,15, 17, 21, 24, 26, 34, 28, 29, 43, 59, 20, 35, 30, 48, 32, 56)
M <- matrix(data, nrow=6)  # data matrix with dimension 6x3
M

# Manual calculation
X <- scale(M)  # scale the matrix M, to ensure data has a mean of 0
               # The standard deviation is also adjusted to 1
round(X, 4)
sX <- svd(X) # do SVD
names(sX)
d <- sX$d
U <- sX$u
V <- sX$v
D <- diag(d)
round(U, 4)
round(V, 4)
round(D, 4)
round(t(V) %*% V, 4)
round(t(U) %*% U, 4)
round(U %*% D %*% t(V), 4)
pcs <- U %*% D
pcs
pcss <- X %*% V
pcss
```

```{r}
# Using pcob
pcob <- prcomp(M, scale=TRUE)  # use R function prcomp, with scaling
                               # Use scaling when the features are very different.
names(pcob)
summary(pcob)
pcob$sdev      # standard deviation from PCs
pcob$rotation  # same as matrix V
pcob$center    # means for scaling
pcob$scale     # sd for scaling
pcob$x        # XV or UD
totalsum <- sum(pcss^2)  # total sum
pc1sum <- sum(pcss[,1]^2) # sum from PC1
pc2sum <- sum(pcss[,2]^2) # sum from PC2
pc3sum <- sum(pcss[,3]^2) # sum from PC3
sd <- sqrt(c(pc1sum, pc2sum, pc3sum)/(6-1)) # n-1=6-1=5 
sd
totalsum
pc1sum
pc2sum
pc3sum
propvar <- c(pc1sum, pc2sum, pc3sum)/totalsum
round(propvar, 4)
par(mfrow=c(1,2))
biplot(pcob, scale=0)
plot(pcob$x[,1], pcob$x[,2], xlab="PC1", ylab="PC2", ylim=c(-2, 2),
     main="Plot PC1 vs. PC2")
```

Using the US Arrest Dataset

```{r}
# Load the USArrests data and set row names as state names
data("USArrests")
states <- row.names(USArrests)
row.names(USArrests) <- states

# Display dimensions and variable names of the dataset
# dim(USArrests)
# names(USArrests)

# Optionally, calculate means and standard deviations for each variable
# apply(USArrests, 2, mean)
# apply(USArrests, 2, sd)

# Perform PCA on the scaled data
pr.out <- prcomp(USArrests, scale = TRUE)
summary(pr.out) # 2 principal components are sufficient

# Examine the components of the PCA output
pr.out$center  # Mean of each variable
pr.out$scale   # Scaling applied to each variable
pr.out$rotation  # The rotation (loading of each variable on the principal components)
```

```{r}
# Dimensions and plot of the first two principal components
dim(pr.out$x)
biplot(pr.out, scale = 0, main = "The first two Principal Components")
```

For the above graph, the top represents PC1 score while the right scale represents PC2 score.

```{r}
# To create a mirrored version of the biplot
## The software may result in different sign for its loading vectors, the rotation converts the sign.
pr.out$rotation <- -pr.out$rotation
pr.out$x <- -pr.out$x
biplot(pr.out, scale = 0, main = "The first two Principal Components mirror image")
```

Explanation of the graph above:

-   Red Arrows:

    -   The direction of an arrow shows how that variable influences the principal components.
    -   The length of an arrow indicates how strongly that variable contributes to the principal components.

-   Interpretation of Loading:

    -   Murder, Assault, and Rape are pointing in the same general direction, meaning they are correlated.
        -   It suggest that Florida seems to have relately high values for Murder, Assault and Rape.
        -   The states on the left side have lower values for murder, assault and rape
    -   UrbanPop is pointing in a different direction, suggesting it has a different relationship with crime rates.

```{r}
# Calculate and print the proportion of variance explained by each principal component
pr.var <- pr.out$sdev^2
pve <- pr.var/sum(pr.var)
round(pve, 4)

# Plotting the variance explained
par(mfrow = c(1, 2))
plot(pve, xlab = "Principal Component", ylab = "Proportion of Variance Explained", ylim = c(0, 1), type = "b")
plot(cumsum(pve), xlab = "Cumulative Principal Component", ylab = "Proportion of Variance Explained", ylim = c(0, 1), type = "b")
```

# **Lesson 3: Unsupervised Learning II** `r emoji::emoji("fishes")`

## Kmeans-clustering

An approach for partitioning a dataset into $K$ distinct, non-overlapping clusters

-   Properties

    -   Each observation belongs to at least one of the K clusters

    -   Clusters are non-overlapping: No observation belongs to more than one cluster

-   Within Cluster Variation

$$
\min_{C_1,...,C_K} \{\sum^K_{k=1} WCV(C_k) \} 
$$

The formula above is to be solved such that the total within-cluster variation, summed over all $K$ clusters, is minimised.

-   Euclidean distance calculation:

$$
WCV(C_k)= \frac{1}{|C_k|} \sum_{i,i'\epsilon C_k} \sum^p_{j=1} (x_{ij}-x_{i'j})^2
$$ where $|C_k|$ denotes the no. of obs in the $k$th cluster.

$(x_{ij}-x_{i'j})^2$ : Represents the distance between two observation

Combining the two formulas above gives us:

$$
\min_{C_1,...,C_K} \{\sum^K_{k=1} \frac{1}{|C_k|} \sum_{i,i'\epsilon C_k} \sum^p_{j=1} (x_{ij}-x_{i'j})^2 \} 
$$ Can be simplied to: $$
2\sum_{i\subset C_k}\sum^{p}_{j=1} (x_{ij}-\bar{x}_{kj})^2
$$

$\bar{x}\_{kj}$ : Represents the mean of the cluster The code chunk below uses the formulas above to deduce the total sum.

The code chunk below manually calculates the formula seen above

```{r}
d1 <- c(12, 13, 14, 17, 20)
d2 <- c(4, 7, 10)
d3 <- c(18, 19, 22, 25, 27)

su1=0
for (i in 1: length(d1)) {
  su1 <- su1 + sum((d1[i]-d1)^2)
}
s1 <- su1/length(d1)

su2=0
for (i in 1: length(d2)) {
  su2 <- su2 + sum((d2[i]-d2)^2)
}
s2 <- su2/length(d2)

su3=0
for (i in 1: length(d3)) {
  su3 <- su3 + sum((d3[i]-d3)^2)
}
s3 <- su3/length(d3)
```

```{r}
s1 ; s2 ; s3
```

```{r}
tots <- s1+s2+s3
tots
```

```{r}
sum1 <- sum((d1-mean(d1))^2)
sum2 <- sum((d2-mean(d2))^2)
sum3 <- sum((d3-mean(d3))^2)
totsum <- 2*(sum1+sum2+sum3)
c(sum1, sum2, sum3, totsum)
```

We see that both formula yield the same total value.

### General steps in K-clustering

1.  Randomly assign a number, from 1 to $K$, to each observation.
    These serve as initial cluster assignments.

2.  Iterate until cluster assignments stop changing:

    2.1: For each of the $K$ clusters, compute the cluster centroid.
    The $k$th cluster centroid is the vector of the $p$ feature means for the observations in the $k$th cluster.

    2.2: Assign each observation to the cluster whose centroid is closest, defined by the Euclidean distance.

Visual Representation of the steps:

![](images/Lesson3_img3.png)

### Example Code of the steps shown above

```{r}
set.seed(2)
x <- matrix(c(12, 13, 11, 18, 16, 17, 19, 20, 21, 22), ncol=2)
x # To view the matrix

plot(x, main="plot of X1 and X2", xlab="X1", ylab="X2", pch=20, cex=2)

k.out <- kmeans(x, centers = 2, nstart=20)  # centers : No. of clusters, nstart: No. of random sets
k.out$cluster

# Plot the Clusters
plot(x, col=(k.out$cluster+1), main="K-means Clustering Results with K=2",
     xlab="X1", ylab="X2", pch=20, cex=2)

# Get the summary of the cluster
k.out
```

Total variation is reduced by 81.2% by clustering them.

**Steps taken from the above example:**

1.  Randomly placed with 2 clusters

```         
-   Cluster 1: 
```

$$
(12,17),\space (16,22)
$$

```         
-   Cluster 2: 
```

$$
(13,19),\space (11,20), \space (18,21)
$$

2.  Calculate the centroid of each cluster

```         
-   Cluster 1: 
```

$$
[\frac{12+16}{2},\space \frac{17+22}{2}] = [14, 19.5]
$$

```         
-   Cluster 2:
```

$$
[\frac{13+11+18}{3},\space \frac{19+20+21}{3}] = [14, 20]
$$

3.  Form the new cluster by comparing the distance from the centroid of each cluster

```         
-   New cluster 1: 
```

$$
(12,17), (13,19)
$$

```         
-   New cluster 2:
```

$$
(16, 22), (11,20), (18,21)
$$

From the above, we see that observation $(13,19)$ is closer to the centroid of cluster 1.
Hence we bring that observation over.
Similarly for observation $(16,22)$ where it is closer to the centroid of cluster 2.
Hence we will bring that observation over.

The steps above will be repeated until no more changes can be made.

**Below is how the variation is manually calculated. We can see that the end result is the same as the above**

```{r}
# Manual calculation of the variation:

g1 <- k.out$centers[1,]
g2 <- k.out$centers[2,]
# calculate total sum of squares
tot <- rep(NA,5)
for (i in 1:5)
  tot[i] <- (x[i,1]-mean(x[,1]))^2+(x[i,2]-mean(x[,2]))^2
sstot <- sum(tot)
sstot
# calculate the sum of squares due to group 1
totg1 <- rep(NA,2)
for (i in 4:5)
  totg1[i-3] <- (x[i,1]-g1[1])^2+(x[i,2]-g1[2])^2
ssg1 <- sum(totg1)
ssg1
# calculate the sum of squares due to group 2
totg2 <- rep(NA,3)
for (i in 1:3)
  totg2[i] <- (x[i,1]-g2[1])^2+(x[i,2]-g2[2])^2
ssg2 <- sum(totg2)
ssg2

# calculate the proportion of sum of squares explained by clusters
1-(ssg1+ssg2)/sstot
```

### Practical Application

#### Initial Visualisation of PCA

```{r}
score <- read.csv("Datasets/Score.csv", stringsAsFactors = TRUE)
summary(score)
dim(score)
attach(score)
# perform PCA
pr.out <- prcomp(score)  # we do not scale the data  
summary(pr.out)
biplot(pr.out, scale=0)
pr.out$rotation
pc1 <- pr.out$x[,1]

plot(pc1, score$Participation)
plot(pc1, score$Assignment)
plot(pc1, score$Test)
plot(pc1, score$Exam)
```

#### K-means clustering

```{r}
set.seed(123)
k.out <- kmeans(score, 4, nstart=20)  # we set 4 clusters
k.out

# Only K-means
plot(score$Exam, score$Test, col=(k.out$cluster+1), 
     main="K-means Clustering Results with K=4 based on Exam and Test scores",
     xlab="Exam", ylab="Test", pch=20, cex=2)

# With PCA
plot(pr.out$x[,1], pr.out$x[,2], col=(k.out$cluster+1), 
     main="K-means Clustering Results with K=4 based on PCA",
     xlab="PC1", ylab="PC2", pch=20, cex=2)
```

## Hierarchical clustering

### Theory

-   Difference to K-Means

    -   Does not require pre-determination of clusters
    -   Results in an attractive tree-based representation
    -   Dendrogram is built starting from the leaves and combining clusters up to the trunk (bottom-up clusters)

-   Methodology (As illustrated below):

    1.  Start with each point in its own cluster
    2.  Identify the closest two clusters and merge them
    3.  Repeat Step 1 and 2
    4.  Ends when all points are in a single cluster

![](images/Lesson3_img1.png)

For the example above,

1.  We start by identifying A and C to be the closest, forming the first part of the Dendrogram
2.  Afterwards, we find the next closest, which is D and E, which forms the second part of the Dendrogram.
3.  Next, cluster AC is closest to B and therefore we merge them together to form one cluster. This is represented in the Dendrogram as well.
4.  Lastly, cluster ACB and cluster DE are merged together, forming the entire Dendrogram

### Types of linkages and its numerical example

```{r, echo = FALSE}
# Create a data frame
data <- data.frame(
  Linkage = c("Complete", "Single", "Average"),
  Description = c("Maximal inter-cluster dissimilarity. Compute all pairwise dissimilarities between the observations in cluster A and the observations in cluster B, and record the largest of these dissimilarities.",
                  "Minimal inter-cluster dissimilarity. Compute all pairwise dissimilarities between the observations. Compute all pairwise dissimilarities between the observations in cluster A and the observations in cluster B, and record the smallest of these dissimilarities.",
                  "Mean inter-cluster dissimilarity. Compute all pairwise dissimilarities between the observations in cluster A and the observations in cluster B, and record the average of these dissimilarities.")
  
)

# Create a table
kable(data, caption = "Types of Linkages")
```

Formal formulas: Complete: $$
d_{CL}(G,H) =\max_{i\space \epsilon \space G,i' \space \epsilon \space H} {d_{ii'}}
$$ Let's say:

-   Cluster 1 has 2 observations: $$
    (2, 4), (3, 5)
    $$

-   Cluster 2 has 2 observations: $$
    (4, 6), (7,6)
    $$

-   Cluster 3 has 1 observation: $$
    (5, 9)
    $$

The inter-cluster dissimilarity between Clusters 1 and 2:

-   (2, 4) and (4, 6) dissimilarity measure =$\sqrt{(2-4)^2+(4-6)^2} = \sqrt{8}$

-   (2, 4) and (7, 6) dissimilarity measure =$\sqrt{(2-7)^2+(4-6)^2 }= \sqrt{29}$

-   (3, 5) and (4, 6) dissimilarity measure =$\sqrt{(3-4)^2+(5-6)^2}=\sqrt{2}$

-   (3, 5) and (7, 6) dissimilarity measure =$\sqrt{(3-7)^2+(5-6)^2}=\sqrt{17}$

The inter-cluster dissimilarity between Clusters 2 and 3:

-   (4, 6) and (5, 9) dissimilarity measure =$\sqrt{(4-5)^2+(6-9)^2} = \sqrt{10}$

-   (7, 6) and (5, 9) dissimilarity measure =$\sqrt{(7-5)^2+(6-9)^2 }= \sqrt{13}$

The inter-cluster dissimilarity between Clusters 1 and 3:

-   (2, 4) and (5, 9) dissimilarity measure =$\sqrt{(2-5)^2+(4-9)^2} = \sqrt{34}$

-   (3, 5) and (5, 9) dissimilarity measure =$\sqrt{(3-5)^2+(5-9)^2 }= \sqrt{20}$

**Complete linkage**: Focus on farthest pair of points, producing tight, compact clusters

Therefore, inter-cluster dissimilarity between Clusters 1 and 2 = $\max(\sqrt{8},\sqrt{29},\sqrt{2},\sqrt{17})=\sqrt{29}$

**Single linkage**: Focuses on closest pair of points, resulting in elongated clusters

Therefore, inter-cluster dissimilarity between Clusters 1 and 2 = $\min(\sqrt{8},\sqrt{29},\sqrt{2},\sqrt{17})=\sqrt{2}$

**Average linkage**: Leads to balanced clustering

Therefore, inter-cluster dissimilarity between Clusters 1 and 2 = $\frac{(\sqrt{8}+\sqrt{29}+\sqrt{2}+\sqrt{17})}{4}=3.4377$

### Additional information:

-   

    ```         
    Correlation-based distance
    ```

    -   If two observations are highly correlated but they have high Euclidean distance, correlation-based distance may be prefered
        -   This measure focuses on the shapes of observation profiles rather than their magnitudes

![](images/Lesson3_img2.png)

### Example Code

```{r}
super <- read.csv("Datasets/Supermarket.csv", stringsAsFactors = TRUE)
summary(super)
# View(super)

set.seed(9876)
k.out <- kmeans(super[,-1], 3, nstart=20) # Remove the customer column
k.out
km.cluster <- k.out$cluster
```

Note that under Dendrogram, the function *cutree(tree, cluster)* helps us cut the tree such that we achieve the desired number of cluster.

**Correlation Distance Graph**

```{r}
dd <- as.dist(1-cor(t(super[,-1])))  # find the correlation-based distance
dd
kkc <- hclust(dd, method="complete")
plot(kkc, main="Complete Linkage with Correlation-Based Distance",
     xlab="", sub="")  
hc.cluster <- cutree(kkc, 3)
hc.cluster
table(km.cluster, hc.cluster)

kks <- hclust(dd, method="single")
plot(kks, main="Single Linkage with Correlation-Based Distance",
     xlab="", sub="")  
cutree(kks, 3)

kka <- hclust(dd, method="average")
plot(kka, main="Average Linkage with Correlation-Based Distance",
     xlab="", sub="")  
cutree(kka, 3)
```

**Euclidean Distance Graph**

```{r}
# Using Euclidean distance
ggc <- hclust(dist(super[,-1]), method="complete")  # use Euclidean distance
plot(ggc, main="Complete Linkage with Euclidean Distance",
     xlab="", sub="")
cutree(ggc, 3)

ggs <- hclust(dist(super[,-1]), method="single")
plot(ggs, main="Single Linkage with Euclidean Distance",
     xlab="", sub="")
cutree(ggs, 3)

gga <- hclust(dist(super[,-1]), method="average")
plot(gga, main="Average Linkage with Euclidean Distance",
     xlab="", sub="")
cutree(gga, 3)
```

### Considerations in Clustering

1.  Should observations first be standardized?
2.  How many clusters should be used for K-means clustering
3.  For hierarchical clustering, what dissimilarity measure, linkage and cutting point of the dendrogram should be used?

# **Lesson 4: Multiple Linear Regression** `r emoji::emoji("rabbit")`

## Basic theory of Regression Model

Note: (Refer to stat learning notes)

$$
Y = \beta_0 +\beta_1X_1
+\beta_2X_2+ ...+\beta_p X_p+\epsilon
$$

-   Interpretation of coefficient

-   Qualitative Variables

    -   Refers to binary variables

-   Interaction variables

    -   Refers to multiplying two variables together to create an interaction effect

-   Non-linear effects

    -   Refers to the use of power terms in the regression to capture non-linear effects

## Dimension Reduction

### Best subset approach

Note: Taken from Stat learning with R

```{r}
library(ISLR)
library(leaps) # Subset package
# Important to remove NA values from dataset
Hit <- na.omit(Hitters)
```

::: panel-tabset
## Best Subset Selection

### Do best subset model

```{r}
regfit2.all <- regsubsets(Salary~., Hit, nvmax=19) # nvmax = 19 as there are 19 variables. Categorical variable counts as total number of levels - 1
reg2.summary <- summary(regfit2.all)
```

### Visualize Plots

```{r}
plot(reg2.summary$rss, main="RSS plot", 
     xlab="Number of variables", ylab="RSS", type="b")
plot(reg2.summary$adjr2, main="Adjusted r^2 plot", 
     xlab="Number of variables", ylab="Adjusted r^2", type="b")
plot(reg2.summary$cp, main="Cp plot", 
     xlab="Number of variables", ylab="Cp", type="b")
plot(reg2.summary$bic, main="BIC plot", 
     xlab="Number of variables", ylab="BIC", type="b")

```

### Pick best model based on selected Criterion

```{r}
a <- which.min(reg2.summary$rss)
b <- which.max(reg2.summary$adjr2)
c <- which.min(reg2.summary$cp)
d <- which.min(reg2.summary$bic)
coef(regfit2.all, a)
coef(regfit2.all, b)
coef(regfit2.all, c)
coef(regfit2.all, d)
```

## Forward Stepwise Selection

### Do best subset model

```{r}
regfit4.all <- regsubsets(Salary~., Hit, nvmax=19,
                          method ="forward") # nvmax = 19 as there are 19 variables. Categorical variable counts as total number of levels - 1
reg4.summary <- summary(regfit4.all)
```

### Visualize Plots

```{r}
plot(reg4.summary$rss, main="RSS plot", 
     xlab="Number of variables", ylab="RSS", type="b")
plot(reg4.summary$adjr2, main="Adjusted r^2 plot", 
     xlab="Number of variables", ylab="Adjusted r^2", type="b")
plot(reg4.summary$cp, main="Cp plot", 
     xlab="Number of variables", ylab="Cp", type="b")
plot(reg4.summary$bic, main="BIC plot", 
     xlab="Number of variables", ylab="BIC", type="b")

```

### Pick best model based on selected Criterion

```{r}
a <- which.min(reg4.summary$rss)
b <- which.max(reg4.summary$adjr2)
c <- which.min(reg4.summary$cp)
d <- which.min(reg4.summary$bic)
coef(regfit4.all, a)
coef(regfit4.all, b)
coef(regfit4.all, c)
coef(regfit4.all, d)
```

## Backward Stepwise Selection

### Do Backward selection model

```{r}
regfit3.all <- regsubsets(Salary~., Hit, nvmax=19,
                          method = "backward") 
# nvmax = 19 as there are 19 variables. Categorical variable counts as total number of levels - 1
reg3.summary <- summary(regfit3.all)
```

### Visualize Plots

```{r}
plot(reg3.summary$rss, main="RSS plot", 
     xlab="Number of variables", ylab="RSS", type="b")
plot(reg3.summary$adjr2, main="Adjusted r^2 plot", 
     xlab="Number of variables", ylab="Adjusted r^2", type="b")
plot(reg3.summary$cp, main="Cp plot", 
     xlab="Number of variables", ylab="Cp", type="b")
plot(reg3.summary$bic, main="BIC plot", 
     xlab="Number of variables", ylab="BIC", type="b")
```

### Pick best model based on selected Criterion

```{r}
a <- which.min(reg3.summary$rss)
b <- which.max(reg3.summary$adjr2)
c <- which.min(reg3.summary$cp)
d <- which.min(reg3.summary$bic)
coef(regfit3.all, a)
coef(regfit3.all, b)
coef(regfit3.all, c)
coef(regfit3.all, d)
```
:::

### Lasso / Ridge

::: panel-tabset
### Lasso

```{r}
# Load the necessary library
library(ISLR)
library(glmnet)

# Load the Hitters dataset
data("Hitters")

# Remove rows with missing values
Hitters <- na.omit(Hitters)

# Create the target variable and predictors
x <- model.matrix(Salary ~ ., Hitters)[, -1]  # Predictor matrix (exclude intercept)
y <- Hitters$Salary  # Response variable

# Perform cross-validation for Lasso regression
set.seed(123)
cv.lasso <- cv.glmnet(x, y, alpha = 1)  # alpha = 1 for Lasso (L1 penalty)

# Plot the cross-validation results
plot(cv.lasso)

# View the optimal lambda from cross-validation
cv.lasso$lambda.min

# Fit the model using the optimal lambda
lasso.model <- glmnet(x, y, alpha = 1, lambda = cv.lasso$lambda.min)

# Predict on the test data (or using a train-test split)
predictions.lasso <- predict(lasso.model, s = cv.lasso$lambda.min, newx = x)

# Evaluate the performance (MSE, RMSE, etc.)
mse.lasso <- mean((predictions.lasso - y)^2)
rmse.lasso <- sqrt(mse.lasso)
print(paste("Lasso RMSE: ", rmse.lasso))
```

### Ridge

```{r}
# Load the necessary library
library(ISLR)
library(glmnet)

# Load the Hitters dataset
data("Hitters")

# Remove rows with missing values
Hitters <- na.omit(Hitters)
# Create the target variable and predictors
x <- model.matrix(Salary ~ ., Hitters)[, -1]  # Predictor matrix (exclude intercept)
y <- Hitters$Salary  # Response variable

# Perform cross-validation for Ridge regression
set.seed(123)
cv.ridge <- cv.glmnet(x, y, alpha = 0)  # alpha = 0 for Ridge (L2 penalty)

# Plot the cross-validation results
plot(cv.ridge)

# View the optimal lambda from cross-validation
cv.ridge$lambda.min

# Fit the model using the optimal lambda
ridge.model <- glmnet(x, y, alpha = 0, lambda = cv.ridge$lambda.min)

# Predict on the test data (or using a train-test split)
predictions.ridge <- predict(ridge.model, s = cv.ridge$lambda.min, newx = x)

# Evaluate the performance (MSE, RMSE, etc.)
mse.ridge <- mean((predictions.ridge - y)^2)
rmse.ridge <- sqrt(mse.ridge)
print(paste("Ridge RMSE: ", rmse.ridge))
```
:::

### PCR regression

#### Theory

-   PCR may perform better: Lower variance but higher bias (When $M<p$)
-   When $M=p$ and no dimension reduction occurs, results are the same as OLS method.

#### Example 1

Used when there are correlated predictors in the model:

The code below does PCA and the regression separately

```{r}
library(pls)
blood <- read.csv("Datasets/BPressure.csv", stringsAsFactors = TRUE)
cor(blood)
```

From the above, we notice that there is high correlation between the features.

-   Basic regression

```{r}
reg.re <- lm(BP~., data=blood)
summary(reg.re)
predict(reg.re, blood)
```

-   Regression with PCR (Uses lm function and pc codes)

```{r}
pr.out <- prcomp(blood[,-1], scale=TRUE) # Removing the BP column
summary(pr.out)
pr.out$x
pr.out$rotation
lmpr <- lm(blood$BP~pr.out$x[,1]) # Using only the first principle component 
summary(lmpr)               
```

-   The code below uses a predefined function for PCR.

```{r}
pcr.re1 <- pcr(BP~., data=blood, scale=TRUE, ncomp=1)
summary(pcr.re1)
pcr.re1$fitted.values
newx1 <- pcr.re1$scores # Obtains the principal component score
```

Take the obtained principal score and do a regression with it.
As shown below:

```{r}
newx1
newreg1 <- lm(blood$BP ~ newx1)
summary(newreg1)
```

If you use M=p, then you go back to the original OLS method.

```{r}
pcr.re2 <- pcr(BP~., data=blood, scale=TRUE, ncomp=2)
summary(pcr.re2)
pcr.re2$fitted.values
newx2 <- pcr.re2$scores
newx2
newreg2 <- lm(blood$BP ~ newx2)
summary(newreg2)
```

Cross compare between the regressions done above.

#### Number of components vs Test MSE:

The larger the number of components: - the **smaller** the **bias** - the **larger** the **variance**

To minimise the MSE in PCR, cross validation is used to determine the number of components.

The code below does the 10fold cross validation as explained earlier:

#### Cross validation for pcr

```{r}
library(ISLR2)
library(pls)
names(Hitters)
dim(Hitters)
sum(is.na(Hitters$Salary))
Hitters <- na.omit(Hitters)
dim(Hitters)

# splitting train and test set
set.seed(1)
train <- sample(1:nrow(Hitters), nrow(Hitters)/2)
test <- (-train)

# run PCR on the training data with Cross Validation
set.seed(1)
pcr.fit <- pcr(Salary~., 
               data=Hitters, 
               subset=train,
               scale=TRUE, 
               validation="CV") # By default, it uses 10 fold validation.
summary(pcr.fit)
validationplot(pcr.fit, val.type="MSEP")
pcr.fit

# we find the lowest CV error when M=5
# Calculate the test MSE with five components
x <- model.matrix(Salary~., Hitters)[,-1]
y <- Hitters$Salary
pcr.pred <- predict(pcr.fit, x[test,], ncomp=5)
mean((pcr.pred-y[test])^2)

# fit PCR on the full data set with M=5
pcr.fitall <- pcr(Salary~., data=Hitters, scale=TRUE, ncomp=5)
summary(pcr.fitall)
```

### Partial Least Squares Method

Difference to PCR:

-   It identifies the new features in a supervised way:

    -   Take each variable of $X_1$ to regress onto each point of $Y_1$, to obtain the coefficient as $\hat{\beta}_{1,1} , \hat{\beta}_{1,2}, ...$
    -   Afterwhich take the residuals of the regression and do it again against $X_2$

```{r}
set.seed(1)
pls.fit <- plsr(Salary~., data=Hitters, subset=train,
                scale=TRUE, validation="CV") 
summary(pls.fit)
validationplot(pls.fit, val.type="MSEP")
# we find the lowest CV error when M=1
# Calculate the test MSE
pls.pred <- predict(pls.fit, x[test,], ncomp=1)
mean((pls.pred-y[test])^2)

# fit PLS on the full data set with M=1
pls.fitall <- plsr(Salary~., data=Hitters, scale=TRUE, ncomp=1)
summary(pls.fitall)
```

Comments:

-   PLS seeks directions that have high variance and high correlation with the response while PCR seeks only the high variance direction

-   PLS often performs no better than ridge regression or PCR

-   PLS is likely to increase variance

**# Lesson 5: Classification** `r emoji::emoji("sheep")`

The theory for the multinomial logistic regression takes from the basic logistic regression.
It uses the form:

$$
p(X) = \frac{e^{\beta_0+\beta_1 X}}{1+e^{\beta_0+\beta_1 X}}
$$ Rearranging the above gives:

$$
log(\frac{p(X)}{1-p(X)}) = \beta_0 +\beta_1 X
$$ As the response variable is binary, we use the maximum likelihood function to estimate its parameters: $$
l(\beta_0, \beta)= \prod_{i:y_i=1} p(x_i)\prod_{i:y_i=0} (1-p(x_i))
$$ The formula above has the same function type as your conditional probability equation.

The above form is used when the response variable $Y$ is binary.

## Multinomial Logistic Regression

The multinomial logistic regression is an extension of the logistic regression where the response variable Y has more than 2 classes.

We first take a single class to serve as our baseline, known as $K$.

$$
log(\frac{Pr(Y=k \space |\space X=x )}{Pr(Y=K \space |\space X=x }) = \beta_{k0} +\beta_{k1}x_1+...+\beta_{kp}x_p
$$

The slide below takes the third class, cc, as the baseline.

![](images/Lesson5_img1.png)

### Code example

```{r}
library(nnet)  # Package for neural networks and multinomial logistic regression
library(broom) # Package for cleaning and structuring model outputs for easier interpretation

mlr <- read.csv("Datasets/MLR.csv", stringsAsFactors = TRUE)
summary(mlr)
```

For this example, there are 3 classes, aa, bb and cc.

-   Multinomial regression code

```{r}
fit.mlr <- multinom(Y~., data=mlr)
# summary(fit.mlr) # To get summary statistics

tidy(fit.mlr, conf.int = TRUE) # Tidies up the summary statistics for easier interpretation
```

```{r}
round(fit.mlr$fitted.values,3) # Gets the predicted probabilities for each class of the dataset used

model <- predict(fit.mlr, mlr) # Gets the predicted classes
model
```

```{r}
# Compares the predicted classes and actual classes
table(model, mlr$Y)
```

row is predicted class while column is the true class

-   When we want to calculate the probability of a certain class

For example we take the 5th observation.

```{r}
mlr[5,]
```

```{r}
# calculate the predicted prob for 5th obs.
bbcoef <- c(-6.733, 0.2555, -1.052, 1.1302)
cccoef <- c(-1.218, 0.1759, -0.6155, 0.3546)
fifth_obs_X <- c(1, 28, 6, 7) # (X value for B0, X1, X2, X3)

den <- 1+exp(sum(bbcoef*fifth_obs_X))+exp(sum(cccoef*fifth_obs_X)) # Denominator formula

proaa <- 1/den
probb <- exp(sum(bbcoef*fifth_obs_X))/den
procc <- exp(sum(cccoef*fifth_obs_X))/den
proaa
probb
procc
```

## Linear and Quadratic Discriminant Analysis

```{r, echo = FALSE}
library(knitr)

# Create a data frame
data <- data.frame(
  Linear_DA = c("Estimates the distribution of X for each class separately, then use Bayes theorem to flip it around to find P(Y | X).", "When classes are well-separated, it is more stable because it models joint distribution of X & Y. If n is small and distribution of X is approximately normal in each classes, LDA estimates are more stable than logistic regression"),
  Logistics_Regression = c("Directly estimates P(Y | X) without modelling the distribution of X.",
                           "When classes are well-separated, parameter estimates are unstable")
)

# Create a table
kable(data, caption = "Linear Discriminant Analysis Vs Logistic Regression")
```

### Theory (Bayes Theorem)

$$
Pr(Y=k \space | \space X=x) = \frac{Pr(X=x \space | \space Y=k)\cdot Pr(Y=k)}{Pr(X=x)}
$$ Suppose that $F_1, F_2, ..., F_n$ are mutually exclusive & exhaustive events such that:

-   Exactly one of the events must occur
-   Their union covers the entire sample space

Now, suppose that event $E$ has occured and we are interested in determining which one of the $F_j$ also occured.
By Bayes formula:

$$
Pr(Y=F_j \space | \space X=E) = \frac{Pr(X=E \space | \space Y=F_j)\cdot Pr(Y=F_j)}{Pr(X=E)}
$$ Simplifying the notations above we have: $$
Pr(F_j \space | \space E) = \frac{Pr(E \space | \space F_j)\cdot Pr(F_j)}{Pr(X=E)}
$$

By definition from above when event $E$ can occur, it happens in conjunction with any one of the event $F_j$.

$$
E = \cup^n_{i=1} EF_i
$$

Then:

$$
P(E) = \sum^n_{i=1} P(EF_i)=\sum^n_{i=1} P(F_i) \times P(E\space | \space F_i)
$$ The above formula is derived from the probability formula: (Event $E$ intersects event $F_i$) $$
P(A \cap B)=P(A|B)\cdot P(B)
$$

Hence, by using the formula, $P(E)$, we have:

$$
Pr(F_j \space | \space E) = \frac{Pr(E \space | \space F_j)\cdot Pr(F_j)}{\sum^n_{i=1} P(F_i) \times P(E\space | \space F_i)}
$$

The general formula for discriminant analysis is:

$$
Pr(Y=k \space | \space X=x)=\frac{\pi_kf_k(x)}{\sum^K_{l=1} \pi_lf_l(x)}
$$ where:

-   $f_k (x) = Pr(X=x \space | \space Y=k)$ is the **density** for $X$ in class $k$.
-   $\pi_k = Pr(Y=k)$ is the marginal or prior probability for class $k$

![](images/Lesson5_img3.png)

For $\pi = 0.5, \pi_2=0.5$,

-   The two classes (green & pink) have equal prior probabilities where mean = 0.

-   For any value below 0, the class to select is green.

For $\pi = 0.3, \pi_2=0.7$,

-   The pink class has a higher prior probability at 0.7.

-   The decision boundary (black dotted line) shifts to the left to favor the pink class

-   Due to the inbalance classification, even if the green density function is slightly higher, the classification may favor the pink class due to higher prior probability.

### Linear Discriminant Analysis (LDA)

The Gaussian density has the form:

$$
f_k(x) = \frac{1}{\sqrt{2\pi} \sigma_k}e^{-\frac{1}{2}(\frac{x-\mu_k}{\sigma_k})^2}
$$ where:

-   $\mu_k$: mean of class $k$
-   $\sigma_k^2$: variance of class $k$ $\rightarrow$ We assume all class share the same variance, $\sigma_k = \sigma$.

Using the Bayes Formula $p_k(x) = Pr(Y=k \space| \space X=x)$:

$$
p_k(x) = \frac{\pi_k \frac{1}{\sqrt{2\pi}\sigma}e^{\frac{1}{2}(\frac{x-\mu_k}{\sigma})^2}}{\sum^K_{l=1} \pi_l \frac{1}{\sqrt{2\pi}\sigma}e^{\frac{1}{2}(\frac{x-\mu_l}{\sigma})^2}}
$$

From the above formula, by taking logs and discarding terms that do no depend on k, we see that this is equivalent to assigning $x$ to the class with the largest discriminant score:

$$
\delta_k(x) = x\cdot \frac{\mu_k}{\sigma^2}-\frac{\mu_k^2}{2\sigma^2}+log(\pi_k)
$$ Essentially, we see that $\delta_k(x)$ is a linear function of x.

Example: If there are 2 classes $(K=2)$ and there is balanced classification $(\pi_1=\pi_2 =0.5)$, then the decision boundary is at :

$$
x = \frac{\mu_1+\mu_2}{2}
$$

![](images/Lesson5_img4.png)

**Estimation of Parameters**

Typically the parameters $(\mu_i , \pi_i, \sigma^2)$ are unknown and therefore we will need to estimate them.

\$\$ \hat{\pi}\_k= \frac{n_k}{n} \\

\hat{\mu}*k =* \frac{1}{n_k} \sum{i: y_i=k} x_i \\

\hat{\sigma} = \frac{1}{n-K}\sum\^K\_{k=1} \sum\_{i: y_i=k} (x_i-\hat{\mu}*k)\^2 =* \sum\^K{k=1} \frac{n_k-1}{n-K}\cdot \hat{\sigma}\_k\^2 \$\$ where:

-   $n_k$: no. of observations in a particulas class, $k$
-   $n$: Total no. of observation

For each value of $Y$, we take the mean,$\mu_i$ of the $X$ features and its $\sum_i$

![](images/Lesson5_img5.png) **This chapter is actually giving me a headache** ``` r emoji::emoji("clown")``r emoji::emoji("clown")``r emoji::emoji("clown") ```

`r emoji::emoji("cry")`

![](images/Lesson5_img6.png)

![](images/Lesson5_img7.png)

#### LDA Coding

-   Understanding the dataset

```{r}
library(ISLR2)
library(MASS) 
summary(Smarket)
dim(Smarket)
```

-   Train test split

```{r}
train <- (Smarket$Year <2005)
SMtrain <- Smarket[train,]
SMtest <- Smarket[!train,]
dim(SMtest)
```

-   Perform LDA

```{r}
y.test <- SMtest$Direction
lda.fit <- lda(Direction ~Lag1 +Lag2, data=Smarket, subset=train)
lda.fit
plot(lda.fit)
```

We see that the plot above approximately supports normal distribution assumption

-   Verify Class Probabilities and mean

```{r}
up <- SMtrain$Direction == "Up"  # Boolean vector for "Up" days
probup <- length(SMtrain$Direction[up]) / length(SMtrain$Direction)  # P(Up)
probup  # Prior probability of "Up"

# Compute mean values of Lag1 and Lag2 for "Up" and "Down" classes
mean(SMtrain$Lag1[!up])  # Mean Lag1 for "Down" class
mean(SMtrain$Lag1[up])   # Mean Lag1 for "Up" class
mean(SMtrain$Lag2[!up])  # Mean Lag2 for "Down" class
mean(SMtrain$Lag2[up])   # Mean Lag2 for "Up" class
```

-   Make predictions on the Test set

```{r}
lda.pred <- predict(lda.fit, SMtest)
lda.class <- lda.pred$class
table(lda.class, y.test)
lda.pred$posterior

```

### Quadratic Discriminant Analysis (QDA)

```{r, echo = FALSE}
library(knitr)

# Create a data frame
data <- data.frame(
  QDA = c("Recommended if training set is very large so that the variance of the classifier is not a major concern."),
  LDA = c("Recommended if there are relatively few training observations and so reducing variance is crucial")
)

# Create a table
kable(data, caption = "Linear Discriminant Analysis Vs Quadratic Discriminant Analysis")
```

#### Performing QDA

```{r}
library(ISLR2)
library(MASS) 
#run the QDA
qda.fit <- qda(Direction ~Lag1 +Lag2, data=Smarket, subset=train)
qda.fit
qda.pred <- predict(qda.fit, SMtest)
qda.class <- qda.pred$class
table(qda.class, y.test)
qda.pred$posterior 
```

# **Lesson 6: Support Vector Machines** `r emoji::emoji("horses")`

A hyperplane has the form:

$$
\beta_0+\beta_1X_1+\beta_2X_2 +... +\beta_pX_p=0
$$

-   Has a subspace of dimension $p-1$ where $p$ is the number of features

![](images/Lesson6_img1.png)

The above graph showcases a one dimensional plane.
A vertical line (hyperplane) can be drawn to separate the blue and red dots.

-   Ideally the distance from the blue dot to the line and the distance from the red dot to the line should be approximately equal. (See topic on margin)

![](images/Lesson6_img2.png)

Example: The hyperplane separates the points:

-   For the blue line, any point on the right is positive while any point on the left is negative

$$
-6+0.8X_1+0.6X_2=0
$$

Looking at Points (7,4) and (2,5), Sub in the points into the above equation

```{r}
-6+0.8*(7)+0.6*4 # Positive
-6+0.8*(2)+0.6*5 # negative
```

## Separating Hyperplane

![](images/Lesson6_img3.png)

Ensure that all purple points are more than 0.
Any deviations would suggest misclassification aka, the hyperplane did not properly separate the two $Y$ observations.

## Margin

-   Denoted as $M$
    -   Distance from the observation to the hyperplane is known as Margin

## Maximal Margin Classifier

-   Maximise the margin (biggest gap) between the two classes

![](images/Lesson6_img4.png)

The points that results in a distance within the margin is not that important.

## Support Vector classifier

What it does is:

-   Greater robustness to individual observations
-   Better classification of most of the training observations.

More often, data points generally do not allow for a clean separation.

This is where we use support vector classifier that maximises a soft margin.

![](images/Lesson6_img5.png)

-   This allows for some misclassification that maximises the margin.
-   If a hard margin is wanted, one will have to sacrifise for a smaller margin.

![](images/Lesson6_img6.png)

-   Point 1 is called the incorrect side of the margin as it passed the margin line.

-   Point 11 is called the incorrect side of the hyperplane.

![](images/Lesson6_img7.png)

-   The parameter $C$ controls for the severity of the classication/margin errors.

![](images/Lesson6_img8.png)

From the graph above:

-   $\epsilon_i=0$: 2, 3, 4, 5, 6,7, 9, 10
-   $0<\epsilon_i<1$: 1, 8
-   $\epsilon_i>1$: 11, 12

Support vectors are: 2, 7, 9

**Budget C**

-   C controls the bias-variance trade-off of the statistical learning technique.
-   When C is small, we seek narrow margins that are rarely violated; this amount to a classifier that is highly fit to the data
-   When C is larger, the margin is wider, and we allow more violations to it. This amount to more bias and less variance
-   Note that observation that lies strictly on the correct side of the margin does not affect the support vector classifier! Only support vectors (lying directly on the margin, or on the wrong side of the margin) do!
-   The support vector classifier's decision rule is based on only on a potentially small subset of the training observations. That means it is quite robust to the behavior of observations that are far away from the hyperplane.

**Why variance is reduced when C is bigger?** Since the margin is much wider, this means more observations becomes the support vector.
If any one of the observation changes, it has little impact on the hyperplane as compared to when there is only two support vector.

**CODING FOR SVM**

The key is to maximise $M$ subject to $\sum^{p}_{j=1} \beta_j^2=1$

![](images/Lesson6_img9.png)

However, in SVM in R, it only takes in the "cost" argument, thereby minimises the expression above.

-   A larger cost parameter $\rightarrow$ Low Budget C
    -   Narrower margin
    -   Fewer support vector
    -   Lower bias and higher variance
-   A smaller cost parameter $\rightarrow$ High Budget C
    -   Larger margin
    -   More support vector
    -   Higher bias and lower variance

## Numeric Example 1:

### Simulation of data and basic logistic regression

```{r}
#Support Vector Classifier example
# numerical example 1

# Simulation of Data
y <- c(rep("aa", 5), rep("bb", 7))
x1<- c(2,1,7,6,4,6,3,5,8,6,5,9)
x2 <-c(5,3,4,1,2,8,4,9,2,6,7,3)
ss<- c(rep(1,5), rep(2,7))
plot(x2,x1, col=ss+1, cex=1.5)
dd3 <- data.frame(y, x1, x2, stringsAsFactors = TRUE)

# Simple logistics regression
log1 <- glm(y~x1+x2, data=dd3, family=binomial)
log1
predict(log1, data=dd3, type="response")
gg <- rep("predict aa", 12)
gg[log1$fitted.values>0.5] <- "Predicted bb"
table(gg, y)
gg
```

### SVM with cost= 0.10

```{r}
library(e1071)
svm1 <- svm(y~x1+x2, data=dd3, kernel="linear", cost=.1, scale=FALSE)
plot(svm1,dd3)
svm1
svm1$index
svm1$fitted
```

### SVM with cost= 10

```{r}
svm2 <- svm(y~x1+x2, data=dd3, kernel="linear", cost=10, scale=FALSE)
plot(svm2,dd3)
svm2
svm2$index
svm2$fitted
```

## Classification with Non-Linear Decision Boundaries (Numeric Example 2)

Purpose: To address **non-linearity** in the model - Enlarge Features with Polynomial Functions - Enlarge the feature space by including quadratic/cubic polynomials.



## Linear Kernel

-   Quantifies the similarity of the two observations

-   Inner Product: Takes the sum of point coordinates 

$$
K(x_i, x_i')=\sum^{p}_{j=1} x_{ij}x_{i'j}
$$ 

For example: $X_1: 4, 7, 6, 9$ $X_2: 2, 5, 4, 6$

$(x_i, x_i')= 4\times2+7\times5+6\times4+9\times6$

$$
f(x)=\beta_0 +\sum^{n}_{i=1} \alpha_i(x,x_i)
$$ 

Most $\alpha$s can be zero as the observation that are away from the separation line do not contribute much to the parameters.

## Numeric Example 2:

### Simulation of data

```{r}
# numerical example 2:
library(e1071)
set.seed(1)
x <- matrix(rnorm(20*2), ncol=2)
y <- c(rep(-1, 10), rep(1, 10))
x[y == 1,] <- x[y ==1, ]+1
plot(x[,2], x[,1], col=3-y)
dat <- data.frame(x=x, y=as.factor(y))
dat
```

### SVM with cost 10

```{r}
svmfit <- svm(y ~ ., data =dat, kernel="linear", cost =10, scale=FALSE)
plot(svmfit, dat)
svmfit$fitted # Shows how SVM classifies the points
svmfit$index
summary(svmfit)
table(svmfit$fitted, y)
```

### SVM with cost 0.1

```{r}
svmfit1 <- svm(y ~ ., data =dat, kernel="linear", cost =0.1, scale=FALSE)
plot(svmfit1, dat)
svmfit$fitted # Shows how SVM classifies the points
svmfit1$index
summary(svmfit1)
table(svmfit1$fitted, y)
```

### Tuning (Cross validation)

```{r}
set.seed(1)
tune.out <- tune(svm, y~., data=dat, kernel="linear", 
                 ranges=list(cost=c(0.001, 0.01, 0.1, 1, 5, 10, 100))) # Testing a range of cost
summary(tune.out)
bestmod <- tune.out$best.model


set.seed(1)
xtest <- matrix(rnorm(20*2), ncol=2)
ytest <- sample(c(-1, 1), 20 , rep=TRUE)
xtest[ytest==1, ] <- xtest[ytest==1,]+1
testdat <- data.frame(x=xtest, y=as.factor(ytest))
ypred <- predict(bestmod, testdat)
table(predict=ypred, truth=testdat$y)
```



## Polynomial Kernel

$$
f(x)=\beta_0+\sum_{i\subset S} \alpha_i K(x,x_i)
$$

$$
K(x_i, x_i')=(1+\sum^p_{j=1} x_{ij}x_{i'j})^d
$$

-   d=1, it is standard linear kernel
-   d\>1, it fits a SVC in a higher dimensional space.

## Radial Kernel

$$
K(x_i,x_{i'})=exp(-\gamma \sum^p_{j=1} (x_{ij}-x_{i'j})^2)
$$

$$
f(x)=\beta_0 +\sum_{i\subset S} \hat{\alpha}_iK(x,x_i)
$$ This Kernel is more concern about the squared distance between two observation.

-   Thereby controls variance by reducing most dimensions severely

![](images/Lesson6_img10.png)

![](images/Lesson6_img11.png)

-   R function svm() will perform multi-class classification using OVO.

## Numeric Example 3:

### Simulation of data

```{r}
# Support Vector Machine
# numerical example 3: radial kernel
set.seed(1)
x <- matrix(rnorm(200*2), ncol=2)
x[1:100,] <- x[1:100, ]+2
x[101:150, ] <- x[101:150,]-2
y <- c(rep(1, 150), rep(2, 50))
dat <- data.frame(x=x, y=as.factor(y))
dat
plot(x[,2], x[,1], col=y)
```

### Train-test split

```{r}
train <- sample(200, 100)
svmfit3 <- svm(y~., data=dat[train,], kernel="radial",
               gamma=1, cost=1)
plot(svmfit3, dat[train,])
```

### Perform CV to select $\gamma$ and cost

```{r}
#perform the cross-validation to select gamma and cost
tune.out <- tune(svm, y~., data=dat[train,],
                 kernel ="radial",
                 ranges=list(
                   cost=c(0.1,1,10,100, 1000),
                   gamma=c(0.5, 1, 2, 3, 4)))
summary(tune.out)
tune.out$best.parameters
bestmod1 <- tune.out$best.model
pred1 <- predict(bestmod1, newdata=dat[-train,])
table(true=dat[-train, "y"], pred1)
```

## Numeric Example 4: (To look into the manual calculation again)
```{r}
# Support Vector Classifier Example
y <- c(rep("aa", 5), rep("bb", 7))
y1 <- c(rep(-1,5), rep(1,7))
x1<- c(2,1,5,6,4,6,3,5,8,6,5,9)
x2 <-c(5,3,4,1,2,8,8,9,7,6,7,3)
ss<- c(rep(1,5), rep(2,7))
plot(x2,x1, col=ss+1, cex=1.5)
dd4 <- data.frame(y, x1, x2, stringsAsFactors = TRUE)
dd4
library(e1071)
svm1 <- svm(y~x1+x2, data=dd4, kernel="linear", cost=1000, scale=FALSE)
plot(svm1,dd4)
svm1
svm1$index
dd4[svm1$index,]
```
The above table forms the equations for the support vector hyperplane:

support vectors (5, 4) (3, 8), (9, 3) all on the margin

$$ 
a+bx_1+cx_2=0 \\

a+5b+4c=-m \\

a+3b+8c=m \\

a+9b+3c=m \\

bb+cc=1 \\
$$

```{r}
c <- sqrt(1/((5/6)^2+1))
b <- 5/6*c
b^2+c^2
a <- -(8*b+12*c)/2
m <- a+3*b+8*c
c(m, a, b, c)
# when cost=1000
# margin=0.8962582, -7.170065+0.6401844X1+0.7682213X2=0
objf1 <- 1/m+1000*0
objf1
```


```{r}
#Cost =0.2
svm2 <- svm(y~x1+x2, data=dd4, kernel="linear", cost=.2, scale=FALSE)
plot(svm2,dd4)
svm2
svm2$index
dd4[svm2$index,]
# support vectors (2, 5) (3, 8), (9, 3) on the margin and
# support vector (5,4) in the wrong side of margin
# a+b*x1+c*x2=0
# a+2b+5c=-m
# a+3b+8c=m
# a+9b+3c=m
# b*b+c*c=1
c <- sqrt(1/((5/6)^2+1))
b <- 5/6*c
b^2+c^2
a <- -(5*b+13*c)/2
m <- a+3*b+8*c
c(m, a, b, c)
# when cost=0.2
# margin=1.472424, -6.593899+0.6401844X1+0.7682213X2=0
mm1 <- (a+2*b+5*c)*y1[svm2$index[1]]
mm2 <- (a+5*b+4*c)*y1[svm2$index[2]]
mm3 <- (a+3*b+8*c)*y1[svm2$index[3]]
mm4 <- (a+9*b+3*c)*y1[svm2$index[4]]
mnew <- c(mm1, mm2, mm3, mm4)
mnew
error <- 1-(mnew/m)
objf2 <- 1/m+.2*sum(error) # min of objective function
error
objf2
```


```{r}
# Cost=0.15
svm3 <- svm(y~x1+x2, data=dd4, kernel="linear", cost=.15, scale=FALSE)
plot(svm3,dd4)
svm3
svm3$index
dd4[svm3$index,]
aa <- -(8*b+11*c)/2
m2 <- aa + 6*b+6*c
c(m2, aa, b, c)
# when cost=0.15
# margin=1.664479, -6.785955+0.6401844X1+0.7682213X2=0
mm1 <- (aa+2*b+5*c)*y1[svm3$index[1]]
mm2 <- (aa+5*b+4*c)*y1[svm3$index[2]]
mm3 <- (aa+3*b+8*c)*y1[svm3$index[3]]
mm4 <- (aa+6*b+6*c)*y1[svm3$index[4]]
mm5 <- (aa+9*b+3*c)*y1[svm3$index[5]]
mnew <- c(mm1, mm2, mm3, mm4, mm5)
error <- 1-(mnew/m2)
objf3 <- 1/m2+.15*sum(error)
c(error, objf3) # We allow for 3 observation to fall on the incorrect side of the margin
c(objf1, objf2, objf3)
```


# **Lesson 7: SVM 2, Naive Bayes Classifer & K-Nearest Neighbors Classifier**

## SVM 2:

-   **SVM 2 is put together with Lesson 6 SVM part**

## Naive Bayes Classifier (NBC)

-   Apply Bayes theorem to classification problem
-   Assume features are independent & equally important within each class
-   Useful when no. of independent variables, $p$ is large

### Theory

-   Generalized Multiplication Rule of Probability

If $E_1, E_2, E_3$ are any arbitrary sets of events then the probability of all events happening is:

$$
P(E_1E_2E_3)=P(E_1)\times P(E_2 |E_1)\times P(E_3|E_1E_2)
$$

Hence, considering $E_1,E_2,...,E_n$ are any arbitrary sets of events, then in general form:

$$
P(E_1E_2...E_n) = P(E_1) \times P(E_2|E_1) \times P(E_3|E_1E_2)\times ...\times P(E_n|E_1E_2...E_{n-1})
$$



![](images/Lesson7_img1.png)

-   NBC Assumptions 

    - Without NBC Assumptions
    
      -   By extension of the above, "Generalized Multiplication Rule of Probability", we have the formula below:
      
    - With NBC Assumptions
    
      -   Simplifies the above formula as it assume independence of all the features. We focus mainly on the numerator.
      -   Assume numerical independent variable to be drawn from a univariate normal distribution
      
$$
      X_j | Y = k ~ N(\mu_{jk},\sigma^2_{jk})      
$$
      
      -   Assume categorical independent variable by their proportion of observations for the predictor corresponding to each class
      
### Numerical Example

-   Explore the dataset values
```{r}
#naive bayes example with qualitative independent variables only
library(MASS)
library(e1071)
spam <- read.csv("Datasets/Spam.csv", stringsAsFactors = TRUE)
summary(spam)
table(spam$Spam, spam$Discount)
table(spam$Spam, spam$Money)
table(spam$Spam, spam$Cheap)
#table(spam$Spam, spam$Discount, spam$Money)
#table(spam$Spam, spam$Discount, spam$Money, spam$Cheap)
```

-   The tables above is made into probabilities via the nb.fit automatically.

```{r}
nb.fit <- naiveBayes(Spam~., data = spam)
nb.fit
```

For example:

$$
P(Spam=No | Discount=No) =78/(2+78)=0.975
$$

-   Checking accuracy of prediction

```{r}
nb.class <- predict(nb.fit, spam)
table(nb.class, spam$Spam)
mean(nb.class == spam$Spam)
```

-   To get the predicted probability of each observation

For example, we look into the 6th and 15th observation:
```{r}
nb.preds <- predict(nb.fit, spam, 
                    type="raw")
nb.class[c(5, 16)] # Real data
nb.preds[c(5, 16),] # Predicted data
spam[c(5,16),]
```
Based on the table above, we can manually calculated the probability. 

For observation 5:

$$
P(Y=Yes) \times P(Y =Yes | Discount = Yes) \times P(Y = Yes | Money = Yes) \times P(Y=Yes | Cheap = No) 
$$

```{r}
# verify the 5th obs. P(Spam=Yes|X)=0.82727
psx <- 0.2*0.25*0.4*0.7
```

$$
P(Y=No) \times P(Y =No | Discount = Yes) \times P(Y = No | Money = Yes) \times P(Y=No | Cheap = No) 
$$

```{r}
pnsx <- 0.8*0.025*0.1625*0.9
```

Hence the probability is:
```{r}
probs5 <- psx/(psx+pnsx)
probs5
```

Do the same for observation 16:

```{r}
# verify the 16th obs. P(Spam=Yes|X)=0.2691014
psx <- 0.2*0.75*0.4*0.7
pnsx <- 0.8*0.975*0.1625*0.9
probs16 <- psx/(psx+pnsx)
probs16

```

Do the same for observation 11:
```{r}
nb.class[11]
nb.preds[11,]
spam[11,]

# verify the 11th obs. P(Spam=No|X)=0.7075548
psx <- 0.2*0.75*0.6*0.3
pnsx <- 0.8*0.975*0.8375*0.1
probs11 <- pnsx/(psx+pnsx)
probs11

```

```{r}
lda.fit <- lda(Spam~., data =spam)
lda.fit
lda.pred <- predict(lda.fit, spam)
lda.class <- lda.pred$class
table(lda.class, spam$Spam)
mean(lda.class == spam$Spam)

lda.pred$posterior[c(5,16),]
table(lda.class, nb.class)

```


### NBC with both qualitative & quantitative features

#### Exploring the dataset
```{r}
#naive bayes with quantitative and qualitative independent variables
library(e1071)
bonus <- read.csv("Datasets/Bonus.csv", stringsAsFactors = TRUE)
summary(bonus)
table(bonus$Bonus, bonus$Local)
table(bonus$Bonus, bonus$Size)
```

#### Calculating mean and standard deviation

-   The mean and sd is used to calculate the probability of Y condition on numerical feature, X.

```{r}
roeNo.mean <- mean(bonus$ROE[bonus$Bonus=="No"])
roeNo.sd <- sd(bonus$ROE[bonus$Bonus=="No"])
roeYes.mean <- mean(bonus$ROE[bonus$Bonus=="Yes"])
roeYes.sd <- sd(bonus$ROE[bonus$Bonus=="Yes"])
c(roeNo.mean,roeNo.sd)
c(roeYes.mean,roeYes.sd)

sgrNo.mean <- mean(bonus$SGR[bonus$Bonus=="No"])
sgrNo.sd <- sd(bonus$SGR[bonus$Bonus=="No"])
sgrYes.mean <- mean(bonus$SGR[bonus$Bonus=="Yes"])
sgrYes.sd <- sd(bonus$SGR[bonus$Bonus=="Yes"])
c(sgrNo.mean, sgrNo.sd)
c(sgrYes.mean, sgrYes.sd)

```

-   **The model results are:**

```{r}
nb.fit <- naiveBayes(Bonus~., data = bonus)
nb.fit
nb.class <- predict(nb.fit, bonus)
table(nb.class, bonus$Bonus)
mean(nb.class == bonus$Bonus)
```

**Note:**

Since ROE and SGR are numeric, the values associated above is the mean and standard deviation of the variable according to the $Y$ class it associates to.


#### To calculate the conditional probability manually

-   We look into observation 20 and 50:

```{r}
nb.preds <- predict(nb.fit, bonus, type="raw")
nb.class[c(20, 50)]
nb.preds[c(20, 50),]
bonus[c(20,50),]
```

```{r}
# verify the 20th obs. P(Bonus=Yes|X)=0.0482301
p_yes <- dnorm(-40.5, 
               nb.fit$table$ROE[2,1], # Mean of ROE for Y = Yes
               nb.fit$table$ROE[2,2]) # SD of ROE for Y = Yes
p_no <- dnorm(-40.5, 
              nb.fit$table$ROE[1,1], # Mean of ROE for Y = No
              nb.fit$table$ROE[1,2]) # SD of ROE for Y = No
proe_yes <- p_yes/(p_yes+p_no)
proe_yes
```

```{r}
p_yes <- dnorm(7.5, 
               nb.fit$table$SGR[2,1], # Mean of SGR for Y = Yes
               nb.fit$table$SGR[2,2]) # Mean of SGR for Y = Yes
p_no <- dnorm(7.5, 
              nb.fit$table$SGR[1,1], # Mean of SGR for Y= No
              nb.fit$table$SGR[1,2]) # SD of SGR for Y = No
psgr_yes <- p_yes/(p_yes+p_no)
psgr_yes
```


**We can then calculate the probability of observation 20's class.**

$$
P(Bonus=Yes) \times P(Bonus =Yes | Local = Yes) \times P(Bonus = Yes | Size = Small) \times P(Bonus =Yes | ROE = -40.5) \times P(Bonus = Yes | SGR = 7.5)
$$
```{r}
psx <- nb.fit$apriori[2]*nb.fit$table$Local[2,2]*nb.fit$table$Size[2,3]*proe_yes*psgr_yes
```

$$
P(Bonus=No) \times P(Bonus =No | Local = Yes) \times P(Bonus = No | Size = Small) \times P(Bonus =No | ROE = -40.5) \times P(Bonus = No | SGR = 7.5)
$$

```{r}
pnsx <- nb.fit$apriori[1]*nb.fit$table$Local[1,2]*nb.fit$table$Size[1,3]*(1-proe_yes)*(1-psgr_yes)
```

```{r}
probs20 <- psx/(psx+pnsx)
probs20
```

### Summary

-   NBC is simple, fast and effective despite relying on often-faulty assumptions

-   Perform well with noisy data

-   Require relatively few examples for training but also works well with large numbers of example

-   Ideal for datasets with many qualitative independent variables

## K-Nearest Neighbors Classifer

### Similarity and distances

-   Euclidean distance

    -   Defined as the straight-line distance between two points in a plane or space

$$
d(x, X_i)=\sqrt{\sum^d_{j=1}(x_j-X_{i_j})^2}
$$

-   Manhattan Distance

    -   Defined as the total distance you would travel if you could move along horizontal & vertical lines (Like in a grid)

$$
d(x,y)=\sum^n_{i=1} |x_i-y_i|
$$

-   Minkowski Distance

    -   Defined as a family of distances which includes both Euclidean and Manhattan distances as special cases

$$
d(x,y)=(\sum^n_{i=1}(x_i-y_i)^p)^{\frac{1}{p}}
$$

### Steps of KNN Classifer

1.  Select the optimal value of K

2.  Calculate distance

    -   To measure the similarity between target and training data points, Euclidean distance is used.
    -   Distance between data points in the dataset and the target point is calculated

3.  Finding nearest neighbors

    -   K data points with the smallest distances to the target point are the nearest neighbors

4.  Voting for classification

    -   These closest points are called neighbors. The algorithm then looks at which category the neighbors belong to and picks the one that appears the most (Majority vote).

Note: Since algorithm relies on distance, scaling (normalising) the training data can greatly improve its accuracy

### Numerical example

```{r}
# K-Nearest Neighbors Classifier
library(class)
library(ISLR2)
# use Smarket data in ISLR2 library
summary(Smarket)
attach(Smarket)
```


```{r}
# Train-test Split
train <- (Smarket$Year <2005) 
train.X <- cbind(Lag1, Lag2)[train,]
test.X <- cbind(Lag1, Lag2)[!train,]
train.Direction <- Direction[train]
Direction.2005 <- Direction[!train]

# Model creation & prediction
set.seed(1)
knn.pred <- knn(train.X, test.X, train.Direction, k=3) # KNN Classifier takes in a matrix form.
table(knn.pred, Direction.2005)
mean(knn.pred == Direction.2005)
```

### Summary

```{r, echo = FALSE}
library(knitr)

# Create a data frame
data <- data.frame(
  Advantages = c("Easy to implement as complexity is relatively low as compared to other ML algorithm", "Stores all data in memory and doesn't require any training such that when new data points are added, it automatically adjust and uses the new data for future predictions", "Few hyperparameters are required in the training of KNN", "Works for both classification and regression"),
  Disadvantages = c("Doesn't scale well with large datasets", 
                    "As the number of features increases, KNN struggles to classify data accurately (Curse of dimensionality)", "Prone to overfitting due to the curse of dimensionality", "-")
)

# Create a table
kable(data, caption = "Advantages & Disadvantages of KNN")
```

# **Lesson 8: Moving Beyond Linearity**

## Polynomial regression

-   Add extra predictors by raising the existing predictors to a power

## Step functions

-   Cut the range of a variable into K distinct regions and then fit a piecewise constant function

## Regression splines

-   Use polynomial regression and step techniques together

## Local regression

-   Similar to splines but regions are allowed to overlap

## Generalize additive models (GAM)

-   Extend above techniques to deal with multiple predictors

# **Lesson 9**

# **Lesson 10**

